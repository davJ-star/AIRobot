{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/mira/anaconda3/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.4.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (2.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchvision) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from jinja2->torch==2.4.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/mira/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: opencv-python in /home/mira/anaconda3/lib/python3.12/site-packages (4.10.0.84)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-1.4.14-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from albumentations) (0.23.2)\n",
      "Requirement already satisfied: PyYAML in /home/mira/anaconda3/lib/python3.12/site-packages (from albumentations) (6.0.1)\n",
      "Collecting pydantic>=2.7.0 (from albumentations)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting albucore>=0.0.13 (from albumentations)\n",
      "  Using cached albucore-0.0.13-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting eval-type-backport (from albumentations)\n",
      "  Using cached eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/mira/anaconda3/lib/python3.12/site-packages (from albucore>=0.0.13->albumentations) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=2.7.0->albumentations)\n",
      "  Using cached pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/mira/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/mira/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in /home/mira/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/mira/anaconda3/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
      "Downloading albumentations-1.4.14-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached albucore-0.0.13-py3-none-any.whl (8.5 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, pydantic-core, opencv-python-headless, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, eval-type-backport, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, albucore, nvidia-cusolver-cu12, albumentations, torch, torchvision\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed albucore-0.0.13 albumentations-1.4.14 eval-type-backport-0.2.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 opencv-python-headless-4.10.0.84 pydantic-2.8.2 pydantic-core-2.20.1 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy torch torchvision opencv-python albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==X.Y.Z+cu121 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==X.Y.Z+cu121\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/mira/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-24.7.1               |  py312h06a4308_0         1.2 MB\n",
      "    pytorch-2.3.0              |cpu_py312h1f09096_0        76.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        77.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pytorch            pkgs/main/linux-64::pytorch-2.3.0-cpu_py312h1f09096_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2024.3.11-h06a4308_0 --> 2024.7.2-h06a4308_0 \n",
      "  certifi                          2024.6.2-py312h06a4308_0 --> 2024.7.4-py312h06a4308_0 \n",
      "  conda                              24.5.0-py312h06a4308_0 --> 24.7.1-py312h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "pytorch-2.3.0        | 76.4 MB   |                                       |   0% \n",
      "pytorch-2.3.0        | 76.4 MB   | 1                                     |   0% \u001b[A\n",
      "conda-24.7.1         | 1.2 MB    | 4                                     |   1% \u001b[A\n",
      "pytorch-2.3.0        | 76.4 MB   | 6                                     |   2% \u001b[A\n",
      "pytorch-2.3.0        | 76.4 MB   | #1                                    |   3% \u001b[A\n",
      "pytorch-2.3.0        | 76.4 MB   | ###                                   |   8% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deeplabv3_resnet50\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:25\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;129m@register_meta\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi_align\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_roi_align\u001b[39m(\u001b[38;5;28minput\u001b[39m, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):\n\u001b[1;32m     27\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(rois\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrois must have shape as Tensor[K, 5]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         ),\n\u001b[1;32m     34\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torchvision\u001b[38;5;241m.\u001b[39mextension\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 (ResNet18을 백본으로 사용)\n",
    "model = deeplabv3_resnet50(weights=None, num_classes=25)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 20 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    108\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()\n\u001b[1;32m    110\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 손실 함수 및 옵티마이저 정의\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:125\u001b[0m, in \u001b[0;36mSequential.__setitem__\u001b[0;34m(self, idx, module)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m, module: Module) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     key: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_by_idx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mkeys(), idx)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:113\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m    111\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m size:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    114\u001b[0m idx \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(islice(iterator, idx, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_resnet50(weights=None, num_classes=25)\n",
    "\n",
    "# 채널 수를 줄이기 위해 첫 번째 컨볼루션 레이어 수정\n",
    "model.backbone.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# ASPP의 채널 수 감소\n",
    "model.classifier[0] = nn.Conv2d(2048, 128, 1, bias=False)\n",
    "model.classifier[1] = nn.BatchNorm2d(128)\n",
    "model.classifier[2] = nn.ReLU()\n",
    "model.classifier[3] = nn.Conv2d(128, 128, 3, padding=1, bias=False)\n",
    "model.classifier[4] = nn.BatchNorm2d(128)\n",
    "model.classifier[5] = nn.ReLU()\n",
    "model.classifier[6] = nn.Conv2d(128, 25, 1)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are missing from the target environment:\n",
      "  - torchvision\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/mira/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ffmpeg-6.1.1               |       h4c62175_0         9.6 MB\n",
      "    leptonica-1.82.0           |       h42c8aad_2         2.5 MB\n",
      "    libogg-1.3.5               |       h27cfd23_1         199 KB\n",
      "    libtheora-1.1.1            |       h7f8727e_3         338 KB\n",
      "    libvorbis-1.3.7            |       h7b6447c_0         398 KB\n",
      "    libvpx-1.13.1              |       h6a678d5_0         1.0 MB\n",
      "    libwebp-1.3.2              |       h11a3e52_0          87 KB\n",
      "    tesseract-5.2.0            |       h6a678d5_0       168.6 MB\n",
      "    torchvision-0.18.1         |cpu_py312h54128f0_0         9.8 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       192.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cairo              pkgs/main/linux-64::cairo-1.16.0-hb05425b_5 \n",
      "  ffmpeg             pkgs/main/linux-64::ffmpeg-6.1.1-h4c62175_0 \n",
      "  graphite2          pkgs/main/linux-64::graphite2-1.3.14-h295c915_1 \n",
      "  harfbuzz           pkgs/main/linux-64::harfbuzz-4.3.0-hf52aaf7_2 \n",
      "  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0 \n",
      "  leptonica          pkgs/main/linux-64::leptonica-1.82.0-h42c8aad_2 \n",
      "  libogg             pkgs/main/linux-64::libogg-1.3.5-h27cfd23_1 \n",
      "  libopus            pkgs/main/linux-64::libopus-1.3.1-h7b6447c_0 \n",
      "  libtheora          pkgs/main/linux-64::libtheora-1.1.1-h7f8727e_3 \n",
      "  libvorbis          pkgs/main/linux-64::libvorbis-1.3.7-h7b6447c_0 \n",
      "  libvpx             pkgs/main/linux-64::libvpx-1.13.1-h6a678d5_0 \n",
      "  libwebp            pkgs/main/linux-64::libwebp-1.3.2-h11a3e52_0 \n",
      "  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0 \n",
      "  pixman             pkgs/main/linux-64::pixman-0.40.0-h7f8727e_1 \n",
      "  tesseract          pkgs/main/linux-64::tesseract-5.2.0-h6a678d5_0 \n",
      "  torchvision        pkgs/main/linux-64::torchvision-0.18.1-cpu_py312h54128f0_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \n",
      "torchvision-0.18.1   | 9.8 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libvpx-1.13.1        | 1.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libvorbis-1.3.7      | 398 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtheora-1.1.1      | 338 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libogg-1.3.5         | 199 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libwebp-1.3.2        | 87 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | ###                                   |   8% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libvpx-1.13.1        | 1.0 MB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | 4                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | ######5                               |  18% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libvpx-1.13.1        | 1.0 MB    | ###########                           |  30% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #4                                    |   4% \u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ###                                   |   8% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | ###############5                      |  42% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libvpx-1.13.1        | 1.0 MB    | ###################################4  |  96% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libvpx-1.13.1        | 1.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libvorbis-1.3.7      | 398 KB    | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | #####################8                |  59% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ####1                                 |  11% \u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | #                                     |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libvorbis-1.3.7      | 398 KB    | ###################################6  |  96% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | ###########################6          |  75% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #####6                                |  15% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtheora-1.1.1      | 338 KB    | #7                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ##6                                   |   7% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libvorbis-1.3.7      | 398 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | #################################4    |  90% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ######8                               |  18% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtheora-1.1.1      | 338 KB    | ###################################   |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ###9                                  |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libtheora-1.1.1      | 338 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libogg-1.3.5         | 199 KB    | ##9                                   |   8% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #######9                              |  22% \u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | #####4                                |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libwebp-1.3.2        | 87 KB     | ######8                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libwebp-1.3.2        | 87 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #########3                            |  25% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libogg-1.3.5         | 199 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libogg-1.3.5         | 199 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ########                              |  22% \u001b[A\n",
      "\n",
      "\n",
      "leptonica-1.82.0     | 2.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ##########5                           |  28% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ###########7                          |  32% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #############2                        |  36% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ###################3                  |  52% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ##############6                       |  40% \u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ######################                |  60% \u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | #########################             |  68% \u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ###################3                  |  52% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ####################7                 |  56% \u001b[A\u001b[A\n",
      "torchvision-0.18.1   | 9.8 MB    | ##############################2       |  82% \u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #######################8              |  64% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  | 1                                     |   0% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | #########################2            |  68% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ##########################8           |  73% \u001b[A\u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  | 3                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  | 5                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "tesseract-5.2.0      | 168.6 MB  | 6                                     |   2% \u001b[A\u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ##################################6   |  94% \u001b[A\u001b[A\n",
      "tesseract-5.2.0      | 168.6 MB  | 9                                     |   3% \u001b[A\n",
      "\n",
      "ffmpeg-6.1.1         | 9.6 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda uninstall pytorch torchvision\n",
    "%conda install pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24, Train Loss: 0.5327, Val Loss: 0.4807\n",
      "Saved best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/24, Train Loss: 0.4548, Val Loss: 0.4737\n",
      "Saved best model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/24, Train Loss: 0.4498, Val Loss: 0.4380\n",
      "Saved best model at epoch 3\n",
      "Saved model at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/24, Train Loss: 0.4333, Val Loss: 0.4982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/24, Train Loss: 0.4247, Val Loss: 0.4546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/24, Train Loss: 0.4142, Val Loss: 0.4035\n",
      "Saved best model at epoch 6\n",
      "Saved model at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/24, Train Loss: 0.3982, Val Loss: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/24, Train Loss: 0.3890, Val Loss: 0.4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/24, Train Loss: 0.3802, Val Loss: 0.3988\n",
      "Saved best model at epoch 9\n",
      "Saved model at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/24, Train Loss: 0.3706, Val Loss: 0.3626\n",
      "Saved best model at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/24, Train Loss: 0.3604, Val Loss: 0.3374\n",
      "Saved best model at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/24, Train Loss: 0.3549, Val Loss: 0.3556\n",
      "Saved model at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/24, Train Loss: 0.3461, Val Loss: 0.3343\n",
      "Saved best model at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/24, Train Loss: 0.3321, Val Loss: 0.3021\n",
      "Saved best model at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/24, Train Loss: 0.3276, Val Loss: 0.3720\n",
      "Saved model at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/24, Train Loss: 0.3185, Val Loss: 0.3086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/24, Train Loss: 0.3071, Val Loss: 0.3146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/24, Train Loss: 0.2981, Val Loss: 0.2978\n",
      "Saved best model at epoch 18\n",
      "Saved model at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/24, Train Loss: 0.2955, Val Loss: 0.2748\n",
      "Saved best model at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/24, Train Loss: 0.2824, Val Loss: 0.2508\n",
      "Saved best model at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/24, Train Loss: 0.2775, Val Loss: 0.2961\n",
      "Saved model at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24, Train Loss: 0.2694, Val Loss: 0.2520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/24, Train Loss: 0.2652, Val Loss: 0.2459\n",
      "Saved best model at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/24, Train Loss: 0.2519, Val Loss: 0.2709\n",
      "Saved model at epoch 24\n",
      "Training complete. Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_resnet50(weights=None, num_classes=25)\n",
    "\n",
    "# 첫 번째 컨볼루션 레이어 수정\n",
    "model.backbone.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# classifier를 새롭게 정의하여 오류 방지\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Conv2d(2048, 256, 1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 25, 1)\n",
    ")\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training complete. Final model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=25)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 13\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training complete. Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"backbone.0.0.weight\", \"backbone.0.1.weight\", \"backbone.0.1.bias\", \"backbone.0.1.running_mean\", \"backbone.0.1.running_var\", \"backbone.1.block.0.0.weight\", \"backbone.1.block.0.1.weight\", \"backbone.1.block.0.1.bias\", \"backbone.1.block.0.1.running_mean\", \"backbone.1.block.0.1.running_var\", \"backbone.1.block.1.0.weight\", \"backbone.1.block.1.1.weight\", \"backbone.1.block.1.1.bias\", \"backbone.1.block.1.1.running_mean\", \"backbone.1.block.1.1.running_var\", \"backbone.2.block.0.0.weight\", \"backbone.2.block.0.1.weight\", \"backbone.2.block.0.1.bias\", \"backbone.2.block.0.1.running_mean\", \"backbone.2.block.0.1.running_var\", \"backbone.2.block.1.0.weight\", \"backbone.2.block.1.1.weight\", \"backbone.2.block.1.1.bias\", \"backbone.2.block.1.1.running_mean\", \"backbone.2.block.1.1.running_var\", \"backbone.2.block.2.0.weight\", \"backbone.2.block.2.1.weight\", \"backbone.2.block.2.1.bias\", \"backbone.2.block.2.1.running_mean\", \"backbone.2.block.2.1.running_var\", \"backbone.3.block.0.0.weight\", \"backbone.3.block.0.1.weight\", \"backbone.3.block.0.1.bias\", \"backbone.3.block.0.1.running_mean\", \"backbone.3.block.0.1.running_var\", \"backbone.3.block.1.0.weight\", \"backbone.3.block.1.1.weight\", \"backbone.3.block.1.1.bias\", \"backbone.3.block.1.1.running_mean\", \"backbone.3.block.1.1.running_var\", \"backbone.3.block.2.0.weight\", \"backbone.3.block.2.1.weight\", \"backbone.3.block.2.1.bias\", \"backbone.3.block.2.1.running_mean\", \"backbone.3.block.2.1.running_var\", \"backbone.4.block.0.0.weight\", \"backbone.4.block.0.1.weight\", \"backbone.4.block.0.1.bias\", \"backbone.4.block.0.1.running_mean\", \"backbone.4.block.0.1.running_var\", \"backbone.4.block.1.0.weight\", \"backbone.4.block.1.1.weight\", \"backbone.4.block.1.1.bias\", \"backbone.4.block.1.1.running_mean\", \"backbone.4.block.1.1.running_var\", \"backbone.4.block.2.fc1.weight\", \"backbone.4.block.2.fc1.bias\", \"backbone.4.block.2.fc2.weight\", \"backbone.4.block.2.fc2.bias\", \"backbone.4.block.3.0.weight\", \"backbone.4.block.3.1.weight\", \"backbone.4.block.3.1.bias\", \"backbone.4.block.3.1.running_mean\", \"backbone.4.block.3.1.running_var\", \"backbone.5.block.0.0.weight\", \"backbone.5.block.0.1.weight\", \"backbone.5.block.0.1.bias\", \"backbone.5.block.0.1.running_mean\", \"backbone.5.block.0.1.running_var\", \"backbone.5.block.1.0.weight\", \"backbone.5.block.1.1.weight\", \"backbone.5.block.1.1.bias\", \"backbone.5.block.1.1.running_mean\", \"backbone.5.block.1.1.running_var\", \"backbone.5.block.2.fc1.weight\", \"backbone.5.block.2.fc1.bias\", \"backbone.5.block.2.fc2.weight\", \"backbone.5.block.2.fc2.bias\", \"backbone.5.block.3.0.weight\", \"backbone.5.block.3.1.weight\", \"backbone.5.block.3.1.bias\", \"backbone.5.block.3.1.running_mean\", \"backbone.5.block.3.1.running_var\", \"backbone.6.block.0.0.weight\", \"backbone.6.block.0.1.weight\", \"backbone.6.block.0.1.bias\", \"backbone.6.block.0.1.running_mean\", \"backbone.6.block.0.1.running_var\", \"backbone.6.block.1.0.weight\", \"backbone.6.block.1.1.weight\", \"backbone.6.block.1.1.bias\", \"backbone.6.block.1.1.running_mean\", \"backbone.6.block.1.1.running_var\", \"backbone.6.block.2.fc1.weight\", \"backbone.6.block.2.fc1.bias\", \"backbone.6.block.2.fc2.weight\", \"backbone.6.block.2.fc2.bias\", \"backbone.6.block.3.0.weight\", \"backbone.6.block.3.1.weight\", \"backbone.6.block.3.1.bias\", \"backbone.6.block.3.1.running_mean\", \"backbone.6.block.3.1.running_var\", \"backbone.7.block.0.0.weight\", \"backbone.7.block.0.1.weight\", \"backbone.7.block.0.1.bias\", \"backbone.7.block.0.1.running_mean\", \"backbone.7.block.0.1.running_var\", \"backbone.7.block.1.0.weight\", \"backbone.7.block.1.1.weight\", \"backbone.7.block.1.1.bias\", \"backbone.7.block.1.1.running_mean\", \"backbone.7.block.1.1.running_var\", \"backbone.7.block.2.0.weight\", \"backbone.7.block.2.1.weight\", \"backbone.7.block.2.1.bias\", \"backbone.7.block.2.1.running_mean\", \"backbone.7.block.2.1.running_var\", \"backbone.8.block.0.0.weight\", \"backbone.8.block.0.1.weight\", \"backbone.8.block.0.1.bias\", \"backbone.8.block.0.1.running_mean\", \"backbone.8.block.0.1.running_var\", \"backbone.8.block.1.0.weight\", \"backbone.8.block.1.1.weight\", \"backbone.8.block.1.1.bias\", \"backbone.8.block.1.1.running_mean\", \"backbone.8.block.1.1.running_var\", \"backbone.8.block.2.0.weight\", \"backbone.8.block.2.1.weight\", \"backbone.8.block.2.1.bias\", \"backbone.8.block.2.1.running_mean\", \"backbone.8.block.2.1.running_var\", \"backbone.9.block.0.0.weight\", \"backbone.9.block.0.1.weight\", \"backbone.9.block.0.1.bias\", \"backbone.9.block.0.1.running_mean\", \"backbone.9.block.0.1.running_var\", \"backbone.9.block.1.0.weight\", \"backbone.9.block.1.1.weight\", \"backbone.9.block.1.1.bias\", \"backbone.9.block.1.1.running_mean\", \"backbone.9.block.1.1.running_var\", \"backbone.9.block.2.0.weight\", \"backbone.9.block.2.1.weight\", \"backbone.9.block.2.1.bias\", \"backbone.9.block.2.1.running_mean\", \"backbone.9.block.2.1.running_var\", \"backbone.10.block.0.0.weight\", \"backbone.10.block.0.1.weight\", \"backbone.10.block.0.1.bias\", \"backbone.10.block.0.1.running_mean\", \"backbone.10.block.0.1.running_var\", \"backbone.10.block.1.0.weight\", \"backbone.10.block.1.1.weight\", \"backbone.10.block.1.1.bias\", \"backbone.10.block.1.1.running_mean\", \"backbone.10.block.1.1.running_var\", \"backbone.10.block.2.0.weight\", \"backbone.10.block.2.1.weight\", \"backbone.10.block.2.1.bias\", \"backbone.10.block.2.1.running_mean\", \"backbone.10.block.2.1.running_var\", \"backbone.11.block.0.0.weight\", \"backbone.11.block.0.1.weight\", \"backbone.11.block.0.1.bias\", \"backbone.11.block.0.1.running_mean\", \"backbone.11.block.0.1.running_var\", \"backbone.11.block.1.0.weight\", \"backbone.11.block.1.1.weight\", \"backbone.11.block.1.1.bias\", \"backbone.11.block.1.1.running_mean\", \"backbone.11.block.1.1.running_var\", \"backbone.11.block.2.fc1.weight\", \"backbone.11.block.2.fc1.bias\", \"backbone.11.block.2.fc2.weight\", \"backbone.11.block.2.fc2.bias\", \"backbone.11.block.3.0.weight\", \"backbone.11.block.3.1.weight\", \"backbone.11.block.3.1.bias\", \"backbone.11.block.3.1.running_mean\", \"backbone.11.block.3.1.running_var\", \"backbone.12.block.0.0.weight\", \"backbone.12.block.0.1.weight\", \"backbone.12.block.0.1.bias\", \"backbone.12.block.0.1.running_mean\", \"backbone.12.block.0.1.running_var\", \"backbone.12.block.1.0.weight\", \"backbone.12.block.1.1.weight\", \"backbone.12.block.1.1.bias\", \"backbone.12.block.1.1.running_mean\", \"backbone.12.block.1.1.running_var\", \"backbone.12.block.2.fc1.weight\", \"backbone.12.block.2.fc1.bias\", \"backbone.12.block.2.fc2.weight\", \"backbone.12.block.2.fc2.bias\", \"backbone.12.block.3.0.weight\", \"backbone.12.block.3.1.weight\", \"backbone.12.block.3.1.bias\", \"backbone.12.block.3.1.running_mean\", \"backbone.12.block.3.1.running_var\", \"backbone.13.block.0.0.weight\", \"backbone.13.block.0.1.weight\", \"backbone.13.block.0.1.bias\", \"backbone.13.block.0.1.running_mean\", \"backbone.13.block.0.1.running_var\", \"backbone.13.block.1.0.weight\", \"backbone.13.block.1.1.weight\", \"backbone.13.block.1.1.bias\", \"backbone.13.block.1.1.running_mean\", \"backbone.13.block.1.1.running_var\", \"backbone.13.block.2.fc1.weight\", \"backbone.13.block.2.fc1.bias\", \"backbone.13.block.2.fc2.weight\", \"backbone.13.block.2.fc2.bias\", \"backbone.13.block.3.0.weight\", \"backbone.13.block.3.1.weight\", \"backbone.13.block.3.1.bias\", \"backbone.13.block.3.1.running_mean\", \"backbone.13.block.3.1.running_var\", \"backbone.14.block.0.0.weight\", \"backbone.14.block.0.1.weight\", \"backbone.14.block.0.1.bias\", \"backbone.14.block.0.1.running_mean\", \"backbone.14.block.0.1.running_var\", \"backbone.14.block.1.0.weight\", \"backbone.14.block.1.1.weight\", \"backbone.14.block.1.1.bias\", \"backbone.14.block.1.1.running_mean\", \"backbone.14.block.1.1.running_var\", \"backbone.14.block.2.fc1.weight\", \"backbone.14.block.2.fc1.bias\", \"backbone.14.block.2.fc2.weight\", \"backbone.14.block.2.fc2.bias\", \"backbone.14.block.3.0.weight\", \"backbone.14.block.3.1.weight\", \"backbone.14.block.3.1.bias\", \"backbone.14.block.3.1.running_mean\", \"backbone.14.block.3.1.running_var\", \"backbone.15.block.0.0.weight\", \"backbone.15.block.0.1.weight\", \"backbone.15.block.0.1.bias\", \"backbone.15.block.0.1.running_mean\", \"backbone.15.block.0.1.running_var\", \"backbone.15.block.1.0.weight\", \"backbone.15.block.1.1.weight\", \"backbone.15.block.1.1.bias\", \"backbone.15.block.1.1.running_mean\", \"backbone.15.block.1.1.running_var\", \"backbone.15.block.2.fc1.weight\", \"backbone.15.block.2.fc1.bias\", \"backbone.15.block.2.fc2.weight\", \"backbone.15.block.2.fc2.bias\", \"backbone.15.block.3.0.weight\", \"backbone.15.block.3.1.weight\", \"backbone.15.block.3.1.bias\", \"backbone.15.block.3.1.running_mean\", \"backbone.15.block.3.1.running_var\", \"backbone.16.0.weight\", \"backbone.16.1.weight\", \"backbone.16.1.bias\", \"backbone.16.1.running_mean\", \"backbone.16.1.running_var\", \"classifier.0.convs.0.0.weight\", \"classifier.0.convs.0.1.weight\", \"classifier.0.convs.0.1.bias\", \"classifier.0.convs.0.1.running_mean\", \"classifier.0.convs.0.1.running_var\", \"classifier.0.convs.1.0.weight\", \"classifier.0.convs.1.1.weight\", \"classifier.0.convs.1.1.bias\", \"classifier.0.convs.1.1.running_mean\", \"classifier.0.convs.1.1.running_var\", \"classifier.0.convs.2.0.weight\", \"classifier.0.convs.2.1.weight\", \"classifier.0.convs.2.1.bias\", \"classifier.0.convs.2.1.running_mean\", \"classifier.0.convs.2.1.running_var\", \"classifier.0.convs.3.0.weight\", \"classifier.0.convs.3.1.weight\", \"classifier.0.convs.3.1.bias\", \"classifier.0.convs.3.1.running_mean\", \"classifier.0.convs.3.1.running_var\", \"classifier.0.convs.4.1.weight\", \"classifier.0.convs.4.2.weight\", \"classifier.0.convs.4.2.bias\", \"classifier.0.convs.4.2.running_mean\", \"classifier.0.convs.4.2.running_var\", \"classifier.0.project.0.weight\", \"classifier.0.project.1.weight\", \"classifier.0.project.1.bias\", \"classifier.0.project.1.running_mean\", \"classifier.0.project.1.running_var\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.2.running_mean\", \"classifier.2.running_var\", \"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.bn1.num_batches_tracked\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.bn1.num_batches_tracked\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.bn2.num_batches_tracked\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.bn3.num_batches_tracked\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.0.downsample.1.num_batches_tracked\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.bn1.num_batches_tracked\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.bn2.num_batches_tracked\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.1.bn3.num_batches_tracked\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.bn1.num_batches_tracked\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.bn2.num_batches_tracked\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer1.2.bn3.num_batches_tracked\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.bn1.num_batches_tracked\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.bn2.num_batches_tracked\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.bn3.num_batches_tracked\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.0.downsample.1.num_batches_tracked\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.bn1.num_batches_tracked\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.bn2.num_batches_tracked\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.1.bn3.num_batches_tracked\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.bn1.num_batches_tracked\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.bn2.num_batches_tracked\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.2.bn3.num_batches_tracked\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.bn1.num_batches_tracked\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.bn2.num_batches_tracked\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer2.3.bn3.num_batches_tracked\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.bn1.num_batches_tracked\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.bn2.num_batches_tracked\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.bn3.num_batches_tracked\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.0.downsample.1.num_batches_tracked\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.bn1.num_batches_tracked\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.bn2.num_batches_tracked\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.1.bn3.num_batches_tracked\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.bn1.num_batches_tracked\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.bn2.num_batches_tracked\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.2.bn3.num_batches_tracked\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.bn1.num_batches_tracked\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.bn2.num_batches_tracked\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.3.bn3.num_batches_tracked\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.bn1.num_batches_tracked\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.bn2.num_batches_tracked\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.4.bn3.num_batches_tracked\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.bn1.num_batches_tracked\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.bn2.num_batches_tracked\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer3.5.bn3.num_batches_tracked\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.bn1.num_batches_tracked\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.bn2.num_batches_tracked\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.bn3.num_batches_tracked\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.0.downsample.1.num_batches_tracked\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.bn1.num_batches_tracked\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.bn2.num_batches_tracked\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.1.bn3.num_batches_tracked\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.bn1.num_batches_tracked\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.bn2.num_batches_tracked\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"backbone.layer4.2.bn3.num_batches_tracked\", \"classifier.6.weight\", \"classifier.6.bias\", \"classifier.0.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.4.running_mean\", \"classifier.4.running_var\", \"classifier.4.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([21, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([21]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 기존의 저장된 모델 가중치 불러오기\u001b[39;00m\n\u001b[1;32m    105\u001b[0m pretrained_weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_autonomous_driving_segmentation_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# .pth 파일 경로\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(pretrained_weights_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# 모델의 출력 레이어 수정 (클래스 수에 맞게 조정)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39min_channels\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"backbone.0.0.weight\", \"backbone.0.1.weight\", \"backbone.0.1.bias\", \"backbone.0.1.running_mean\", \"backbone.0.1.running_var\", \"backbone.1.block.0.0.weight\", \"backbone.1.block.0.1.weight\", \"backbone.1.block.0.1.bias\", \"backbone.1.block.0.1.running_mean\", \"backbone.1.block.0.1.running_var\", \"backbone.1.block.1.0.weight\", \"backbone.1.block.1.1.weight\", \"backbone.1.block.1.1.bias\", \"backbone.1.block.1.1.running_mean\", \"backbone.1.block.1.1.running_var\", \"backbone.2.block.0.0.weight\", \"backbone.2.block.0.1.weight\", \"backbone.2.block.0.1.bias\", \"backbone.2.block.0.1.running_mean\", \"backbone.2.block.0.1.running_var\", \"backbone.2.block.1.0.weight\", \"backbone.2.block.1.1.weight\", \"backbone.2.block.1.1.bias\", \"backbone.2.block.1.1.running_mean\", \"backbone.2.block.1.1.running_var\", \"backbone.2.block.2.0.weight\", \"backbone.2.block.2.1.weight\", \"backbone.2.block.2.1.bias\", \"backbone.2.block.2.1.running_mean\", \"backbone.2.block.2.1.running_var\", \"backbone.3.block.0.0.weight\", \"backbone.3.block.0.1.weight\", \"backbone.3.block.0.1.bias\", \"backbone.3.block.0.1.running_mean\", \"backbone.3.block.0.1.running_var\", \"backbone.3.block.1.0.weight\", \"backbone.3.block.1.1.weight\", \"backbone.3.block.1.1.bias\", \"backbone.3.block.1.1.running_mean\", \"backbone.3.block.1.1.running_var\", \"backbone.3.block.2.0.weight\", \"backbone.3.block.2.1.weight\", \"backbone.3.block.2.1.bias\", \"backbone.3.block.2.1.running_mean\", \"backbone.3.block.2.1.running_var\", \"backbone.4.block.0.0.weight\", \"backbone.4.block.0.1.weight\", \"backbone.4.block.0.1.bias\", \"backbone.4.block.0.1.running_mean\", \"backbone.4.block.0.1.running_var\", \"backbone.4.block.1.0.weight\", \"backbone.4.block.1.1.weight\", \"backbone.4.block.1.1.bias\", \"backbone.4.block.1.1.running_mean\", \"backbone.4.block.1.1.running_var\", \"backbone.4.block.2.fc1.weight\", \"backbone.4.block.2.fc1.bias\", \"backbone.4.block.2.fc2.weight\", \"backbone.4.block.2.fc2.bias\", \"backbone.4.block.3.0.weight\", \"backbone.4.block.3.1.weight\", \"backbone.4.block.3.1.bias\", \"backbone.4.block.3.1.running_mean\", \"backbone.4.block.3.1.running_var\", \"backbone.5.block.0.0.weight\", \"backbone.5.block.0.1.weight\", \"backbone.5.block.0.1.bias\", \"backbone.5.block.0.1.running_mean\", \"backbone.5.block.0.1.running_var\", \"backbone.5.block.1.0.weight\", \"backbone.5.block.1.1.weight\", \"backbone.5.block.1.1.bias\", \"backbone.5.block.1.1.running_mean\", \"backbone.5.block.1.1.running_var\", \"backbone.5.block.2.fc1.weight\", \"backbone.5.block.2.fc1.bias\", \"backbone.5.block.2.fc2.weight\", \"backbone.5.block.2.fc2.bias\", \"backbone.5.block.3.0.weight\", \"backbone.5.block.3.1.weight\", \"backbone.5.block.3.1.bias\", \"backbone.5.block.3.1.running_mean\", \"backbone.5.block.3.1.running_var\", \"backbone.6.block.0.0.weight\", \"backbone.6.block.0.1.weight\", \"backbone.6.block.0.1.bias\", \"backbone.6.block.0.1.running_mean\", \"backbone.6.block.0.1.running_var\", \"backbone.6.block.1.0.weight\", \"backbone.6.block.1.1.weight\", \"backbone.6.block.1.1.bias\", \"backbone.6.block.1.1.running_mean\", \"backbone.6.block.1.1.running_var\", \"backbone.6.block.2.fc1.weight\", \"backbone.6.block.2.fc1.bias\", \"backbone.6.block.2.fc2.weight\", \"backbone.6.block.2.fc2.bias\", \"backbone.6.block.3.0.weight\", \"backbone.6.block.3.1.weight\", \"backbone.6.block.3.1.bias\", \"backbone.6.block.3.1.running_mean\", \"backbone.6.block.3.1.running_var\", \"backbone.7.block.0.0.weight\", \"backbone.7.block.0.1.weight\", \"backbone.7.block.0.1.bias\", \"backbone.7.block.0.1.running_mean\", \"backbone.7.block.0.1.running_var\", \"backbone.7.block.1.0.weight\", \"backbone.7.block.1.1.weight\", \"backbone.7.block.1.1.bias\", \"backbone.7.block.1.1.running_mean\", \"backbone.7.block.1.1.running_var\", \"backbone.7.block.2.0.weight\", \"backbone.7.block.2.1.weight\", \"backbone.7.block.2.1.bias\", \"backbone.7.block.2.1.running_mean\", \"backbone.7.block.2.1.running_var\", \"backbone.8.block.0.0.weight\", \"backbone.8.block.0.1.weight\", \"backbone.8.block.0.1.bias\", \"backbone.8.block.0.1.running_mean\", \"backbone.8.block.0.1.running_var\", \"backbone.8.block.1.0.weight\", \"backbone.8.block.1.1.weight\", \"backbone.8.block.1.1.bias\", \"backbone.8.block.1.1.running_mean\", \"backbone.8.block.1.1.running_var\", \"backbone.8.block.2.0.weight\", \"backbone.8.block.2.1.weight\", \"backbone.8.block.2.1.bias\", \"backbone.8.block.2.1.running_mean\", \"backbone.8.block.2.1.running_var\", \"backbone.9.block.0.0.weight\", \"backbone.9.block.0.1.weight\", \"backbone.9.block.0.1.bias\", \"backbone.9.block.0.1.running_mean\", \"backbone.9.block.0.1.running_var\", \"backbone.9.block.1.0.weight\", \"backbone.9.block.1.1.weight\", \"backbone.9.block.1.1.bias\", \"backbone.9.block.1.1.running_mean\", \"backbone.9.block.1.1.running_var\", \"backbone.9.block.2.0.weight\", \"backbone.9.block.2.1.weight\", \"backbone.9.block.2.1.bias\", \"backbone.9.block.2.1.running_mean\", \"backbone.9.block.2.1.running_var\", \"backbone.10.block.0.0.weight\", \"backbone.10.block.0.1.weight\", \"backbone.10.block.0.1.bias\", \"backbone.10.block.0.1.running_mean\", \"backbone.10.block.0.1.running_var\", \"backbone.10.block.1.0.weight\", \"backbone.10.block.1.1.weight\", \"backbone.10.block.1.1.bias\", \"backbone.10.block.1.1.running_mean\", \"backbone.10.block.1.1.running_var\", \"backbone.10.block.2.0.weight\", \"backbone.10.block.2.1.weight\", \"backbone.10.block.2.1.bias\", \"backbone.10.block.2.1.running_mean\", \"backbone.10.block.2.1.running_var\", \"backbone.11.block.0.0.weight\", \"backbone.11.block.0.1.weight\", \"backbone.11.block.0.1.bias\", \"backbone.11.block.0.1.running_mean\", \"backbone.11.block.0.1.running_var\", \"backbone.11.block.1.0.weight\", \"backbone.11.block.1.1.weight\", \"backbone.11.block.1.1.bias\", \"backbone.11.block.1.1.running_mean\", \"backbone.11.block.1.1.running_var\", \"backbone.11.block.2.fc1.weight\", \"backbone.11.block.2.fc1.bias\", \"backbone.11.block.2.fc2.weight\", \"backbone.11.block.2.fc2.bias\", \"backbone.11.block.3.0.weight\", \"backbone.11.block.3.1.weight\", \"backbone.11.block.3.1.bias\", \"backbone.11.block.3.1.running_mean\", \"backbone.11.block.3.1.running_var\", \"backbone.12.block.0.0.weight\", \"backbone.12.block.0.1.weight\", \"backbone.12.block.0.1.bias\", \"backbone.12.block.0.1.running_mean\", \"backbone.12.block.0.1.running_var\", \"backbone.12.block.1.0.weight\", \"backbone.12.block.1.1.weight\", \"backbone.12.block.1.1.bias\", \"backbone.12.block.1.1.running_mean\", \"backbone.12.block.1.1.running_var\", \"backbone.12.block.2.fc1.weight\", \"backbone.12.block.2.fc1.bias\", \"backbone.12.block.2.fc2.weight\", \"backbone.12.block.2.fc2.bias\", \"backbone.12.block.3.0.weight\", \"backbone.12.block.3.1.weight\", \"backbone.12.block.3.1.bias\", \"backbone.12.block.3.1.running_mean\", \"backbone.12.block.3.1.running_var\", \"backbone.13.block.0.0.weight\", \"backbone.13.block.0.1.weight\", \"backbone.13.block.0.1.bias\", \"backbone.13.block.0.1.running_mean\", \"backbone.13.block.0.1.running_var\", \"backbone.13.block.1.0.weight\", \"backbone.13.block.1.1.weight\", \"backbone.13.block.1.1.bias\", \"backbone.13.block.1.1.running_mean\", \"backbone.13.block.1.1.running_var\", \"backbone.13.block.2.fc1.weight\", \"backbone.13.block.2.fc1.bias\", \"backbone.13.block.2.fc2.weight\", \"backbone.13.block.2.fc2.bias\", \"backbone.13.block.3.0.weight\", \"backbone.13.block.3.1.weight\", \"backbone.13.block.3.1.bias\", \"backbone.13.block.3.1.running_mean\", \"backbone.13.block.3.1.running_var\", \"backbone.14.block.0.0.weight\", \"backbone.14.block.0.1.weight\", \"backbone.14.block.0.1.bias\", \"backbone.14.block.0.1.running_mean\", \"backbone.14.block.0.1.running_var\", \"backbone.14.block.1.0.weight\", \"backbone.14.block.1.1.weight\", \"backbone.14.block.1.1.bias\", \"backbone.14.block.1.1.running_mean\", \"backbone.14.block.1.1.running_var\", \"backbone.14.block.2.fc1.weight\", \"backbone.14.block.2.fc1.bias\", \"backbone.14.block.2.fc2.weight\", \"backbone.14.block.2.fc2.bias\", \"backbone.14.block.3.0.weight\", \"backbone.14.block.3.1.weight\", \"backbone.14.block.3.1.bias\", \"backbone.14.block.3.1.running_mean\", \"backbone.14.block.3.1.running_var\", \"backbone.15.block.0.0.weight\", \"backbone.15.block.0.1.weight\", \"backbone.15.block.0.1.bias\", \"backbone.15.block.0.1.running_mean\", \"backbone.15.block.0.1.running_var\", \"backbone.15.block.1.0.weight\", \"backbone.15.block.1.1.weight\", \"backbone.15.block.1.1.bias\", \"backbone.15.block.1.1.running_mean\", \"backbone.15.block.1.1.running_var\", \"backbone.15.block.2.fc1.weight\", \"backbone.15.block.2.fc1.bias\", \"backbone.15.block.2.fc2.weight\", \"backbone.15.block.2.fc2.bias\", \"backbone.15.block.3.0.weight\", \"backbone.15.block.3.1.weight\", \"backbone.15.block.3.1.bias\", \"backbone.15.block.3.1.running_mean\", \"backbone.15.block.3.1.running_var\", \"backbone.16.0.weight\", \"backbone.16.1.weight\", \"backbone.16.1.bias\", \"backbone.16.1.running_mean\", \"backbone.16.1.running_var\", \"classifier.0.convs.0.0.weight\", \"classifier.0.convs.0.1.weight\", \"classifier.0.convs.0.1.bias\", \"classifier.0.convs.0.1.running_mean\", \"classifier.0.convs.0.1.running_var\", \"classifier.0.convs.1.0.weight\", \"classifier.0.convs.1.1.weight\", \"classifier.0.convs.1.1.bias\", \"classifier.0.convs.1.1.running_mean\", \"classifier.0.convs.1.1.running_var\", \"classifier.0.convs.2.0.weight\", \"classifier.0.convs.2.1.weight\", \"classifier.0.convs.2.1.bias\", \"classifier.0.convs.2.1.running_mean\", \"classifier.0.convs.2.1.running_var\", \"classifier.0.convs.3.0.weight\", \"classifier.0.convs.3.1.weight\", \"classifier.0.convs.3.1.bias\", \"classifier.0.convs.3.1.running_mean\", \"classifier.0.convs.3.1.running_var\", \"classifier.0.convs.4.1.weight\", \"classifier.0.convs.4.2.weight\", \"classifier.0.convs.4.2.bias\", \"classifier.0.convs.4.2.running_mean\", \"classifier.0.convs.4.2.running_var\", \"classifier.0.project.0.weight\", \"classifier.0.project.1.weight\", \"classifier.0.project.1.bias\", \"classifier.0.project.1.running_mean\", \"classifier.0.project.1.running_var\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.2.running_mean\", \"classifier.2.running_var\", \"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.bn1.num_batches_tracked\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.bn1.num_batches_tracked\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.bn2.num_batches_tracked\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.bn3.num_batches_tracked\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.0.downsample.1.num_batches_tracked\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.bn1.num_batches_tracked\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.bn2.num_batches_tracked\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.1.bn3.num_batches_tracked\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.bn1.num_batches_tracked\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.bn2.num_batches_tracked\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer1.2.bn3.num_batches_tracked\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.bn1.num_batches_tracked\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.bn2.num_batches_tracked\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.bn3.num_batches_tracked\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.0.downsample.1.num_batches_tracked\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.bn1.num_batches_tracked\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.bn2.num_batches_tracked\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.1.bn3.num_batches_tracked\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.bn1.num_batches_tracked\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.bn2.num_batches_tracked\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.2.bn3.num_batches_tracked\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.bn1.num_batches_tracked\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.bn2.num_batches_tracked\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer2.3.bn3.num_batches_tracked\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.bn1.num_batches_tracked\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.bn2.num_batches_tracked\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.bn3.num_batches_tracked\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.0.downsample.1.num_batches_tracked\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.bn1.num_batches_tracked\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.bn2.num_batches_tracked\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.1.bn3.num_batches_tracked\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.bn1.num_batches_tracked\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.bn2.num_batches_tracked\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.2.bn3.num_batches_tracked\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.bn1.num_batches_tracked\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.bn2.num_batches_tracked\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.3.bn3.num_batches_tracked\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.bn1.num_batches_tracked\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.bn2.num_batches_tracked\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.4.bn3.num_batches_tracked\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.bn1.num_batches_tracked\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.bn2.num_batches_tracked\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer3.5.bn3.num_batches_tracked\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.bn1.num_batches_tracked\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.bn2.num_batches_tracked\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.bn3.num_batches_tracked\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.0.downsample.1.num_batches_tracked\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.bn1.num_batches_tracked\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.bn2.num_batches_tracked\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.1.bn3.num_batches_tracked\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.bn1.num_batches_tracked\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.bn2.num_batches_tracked\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"backbone.layer4.2.bn3.num_batches_tracked\", \"classifier.6.weight\", \"classifier.6.bias\", \"classifier.0.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.4.running_mean\", \"classifier.4.running_var\", \"classifier.4.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([21, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([21])."
     ]
    }
   ],
   "source": [
    "'''import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        # 데이터 증강 적용\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 전처리 및 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.5),  # 회전: -15도에서 15도까지\n",
    "    A.HorizontalFlip(p=0.5),  # 수평 뒤집기\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # 밝기 조절\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),  # 가우시안 노이즈 추가\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(1, 5), p=0.5),  # 가우시안 블러\n",
    "        A.MedianBlur(blur_limit=3, p=0.5)  # 미디안 블러\n",
    "    ], p=0.3), \n",
    "    A.OneOf([\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.Resize(256, 256),  # 이미지 크기 조정\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "    ToTensorV2(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 전이 학습을 위한 수정\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=True)  # 사전 학습된 모델 로드\n",
    "\n",
    "# 기존의 저장된 모델 가중치 불러오기\n",
    "pretrained_weights_path = 'final_autonomous_driving_segmentation_model.pth'  # .pth 파일 경로\n",
    "model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# 모델의 출력 레이어 수정 (클래스 수에 맞게 조정)\n",
    "num_ftrs = model.classifier[4].in_channels\n",
    "model.classifier[4] = nn.Conv2d(num_ftrs, 25, kernel_size=(1, 1))  # 클래스 수에 맞게 조정\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# 혼합 정밀도 학습 스케일러 정의\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    scheduler.step()  # 학습률 스케줄러 업데이트\n",
    "\n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training complete. Final model saved.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
