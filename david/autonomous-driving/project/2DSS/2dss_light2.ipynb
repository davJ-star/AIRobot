{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=25)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training complete. Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mira/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mira/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24, Train Loss: 0.4277, Val Loss: 0.3085\n",
      "Saved best model at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/24, Train Loss: 0.2865, Val Loss: 0.2624\n",
      "Saved best model at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/24, Train Loss: 0.2246, Val Loss: 0.2012\n",
      "Saved best model at epoch 3\n",
      "Saved model at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/24, Train Loss: 0.1996, Val Loss: 0.1760\n",
      "Saved best model at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/24, Train Loss: 0.1842, Val Loss: 0.1423\n",
      "Saved best model at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/24, Train Loss: 0.1605, Val Loss: 0.1446\n",
      "Saved model at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/24, Train Loss: 0.1448, Val Loss: 0.1537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/24, Train Loss: 0.1363, Val Loss: 0.1132\n",
      "Saved best model at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/24, Train Loss: 0.1266, Val Loss: 0.1332\n",
      "Saved model at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/24, Train Loss: 0.1158, Val Loss: 0.1007\n",
      "Saved best model at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/24, Train Loss: 0.1150, Val Loss: 0.1468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/24, Train Loss: 0.1084, Val Loss: 0.0934\n",
      "Saved best model at epoch 12\n",
      "Saved model at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/24, Train Loss: 0.1041, Val Loss: 0.0933\n",
      "Saved best model at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/24, Train Loss: 0.1048, Val Loss: 0.1560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/24, Train Loss: 0.0968, Val Loss: 0.0943\n",
      "Saved model at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/24, Train Loss: 0.1019, Val Loss: 0.1011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/24, Train Loss: 0.0883, Val Loss: 0.0779\n",
      "Saved best model at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/24, Train Loss: 0.0860, Val Loss: 0.0856\n",
      "Saved model at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/24, Train Loss: 0.0865, Val Loss: 0.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/24, Train Loss: 0.0822, Val Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/24, Train Loss: 0.0804, Val Loss: 0.0746\n",
      "Saved best model at epoch 21\n",
      "Saved model at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24, Train Loss: 0.0809, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/24, Train Loss: 0.0748, Val Loss: 0.0805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "Premature end of JPEG file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/24, Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Saved model at epoch 24\n",
      "Training complete. Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        # 데이터 증강 적용\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 전처리 및 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.5),  # 회전: -15도에서 15도까지\n",
    "    A.HorizontalFlip(p=0.5),  # 수평 뒤집기\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # 밝기 조절\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),  # 가우시안 노이즈 추가\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(1, 5), p=0.5),  # 가우시안 블러 (정수 범위로 수정)\n",
    "        A.MedianBlur(blur_limit=3, p=0.5)  # 미디안 블러\n",
    "    ], p=0.3), \n",
    "    A.OneOf([\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.Resize(256, 256),  # 이미지 크기 조정\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "    ToTensorV2(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=25)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Training complete. Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset train/val/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Premature end of JPEG file\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd271246480>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd271246480>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/mira/anaconda3/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 177\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, optimizer, device)\n\u001b[1;32m    178\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)  \u001b[38;5;66;03m# 검증 데이터셋 사용\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 129\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[0;32m--> 129\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    130\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    132\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_files, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = image_files\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        # 데이터 증강 적용\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 전처리 및 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.5),  # 회전: -15도에서 15도까지\n",
    "    A.HorizontalFlip(p=0.5),  # 수평 뒤집기\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),  # 밝기 조절\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),  # 가우시안 노이즈 추가\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(1, 5), p=0.5),  # 가우시안 블러 (정수 범위로 수정)\n",
    "        A.MedianBlur(blur_limit=3, p=0.5)  # 미디안 블러\n",
    "    ], p=0.3), \n",
    "    A.OneOf([\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.Resize(256, 256),  # 이미지 크기 조정\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 이미지 정규화\n",
    "    ToTensorV2(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터셋 생성\n",
    "dataset = AutonomousDrivingDataset(root_dir='training', image_files=sorted(os.listdir(os.path.join('training', 'images'))))\n",
    "\n",
    "# 데이터셋 나누기\n",
    "train_files, val_files = train_test_split(dataset.image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# 학습용 데이터셋\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', image_files=train_files, original_transform=original_transform, augment_transform=train_transform)\n",
    "\n",
    "# 검증용 데이터셋\n",
    "val_dataset = AutonomousDrivingDataset(root_dir='training', image_files=val_files, original_transform=original_transform, augment_transform=None)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 수정\n",
    "model = deeplabv3_mobilenet_v3_large(pretrained=False, num_classes=25)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 24\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)  # 검증 데이터셋 사용\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_autonomous_driving_segmentation_model_split.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'autonomous_driving_segmentation_model_split_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_autonomous_driving_segmentation_split_model.pth')\n",
    "print(\"Training complete. Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 238\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# 데이터셋 및 데이터로더 생성\u001b[39;00m\n\u001b[1;32m    233\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    234\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    235\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m    236\u001b[0m ])\n\u001b[0;32m--> 238\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDataset(image_dir, mask_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m    239\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m train_test_split(dataset, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    240\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 204\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, image_dir, mask_dir, transform)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir \u001b[38;5;241m=\u001b[39m mask_dir\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(image_dir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/images'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        mask = torch.from_numpy(np.array(mask)).long()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# 데이터 경로 및 하이퍼파라미터 설정\n",
    "image_dir = \"path/to/images\"\n",
    "mask_dir = \"path/to/masks\"\n",
    "batch_size = 8\n",
    "num_classes = 25\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(image_dir, mask_dir, transform=transform)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 정의 및 저장된 가중치 불러오기\n",
    "model = deeplabv3_mobilenet_v3_large(weights=None, num_classes=num_classes)\n",
    "# 불러오는 가중치의 키가 모델의 키와 일치하는지 확인\n",
    "model.load_state_dict(torch.load('best_autonomous_driving_segmentation_model.pth'), strict=False)\n",
    "\n",
    "# 일부 레이어만 학습 가능하도록 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 필요한 레이어만 학습 가능하도록 설정 (예: 마지막 레이어)\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 모델 검증\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, masks in val_loader:\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        val_loss += loss.item()\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "# 학습된 모델 저장\n",
    "torch.save(model.state_dict(), f'finetuned_autonomous_driving_segmentation_model{1}.pth')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3425020/4226293784.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_autonomous_driving_segmentation_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"backbone.0.0.weight\", \"backbone.0.1.weight\", \"backbone.0.1.bias\", \"backbone.0.1.running_mean\", \"backbone.0.1.running_var\", \"backbone.1.block.0.0.weight\", \"backbone.1.block.0.1.weight\", \"backbone.1.block.0.1.bias\", \"backbone.1.block.0.1.running_mean\", \"backbone.1.block.0.1.running_var\", \"backbone.1.block.1.0.weight\", \"backbone.1.block.1.1.weight\", \"backbone.1.block.1.1.bias\", \"backbone.1.block.1.1.running_mean\", \"backbone.1.block.1.1.running_var\", \"backbone.2.block.0.0.weight\", \"backbone.2.block.0.1.weight\", \"backbone.2.block.0.1.bias\", \"backbone.2.block.0.1.running_mean\", \"backbone.2.block.0.1.running_var\", \"backbone.2.block.1.0.weight\", \"backbone.2.block.1.1.weight\", \"backbone.2.block.1.1.bias\", \"backbone.2.block.1.1.running_mean\", \"backbone.2.block.1.1.running_var\", \"backbone.2.block.2.0.weight\", \"backbone.2.block.2.1.weight\", \"backbone.2.block.2.1.bias\", \"backbone.2.block.2.1.running_mean\", \"backbone.2.block.2.1.running_var\", \"backbone.3.block.0.0.weight\", \"backbone.3.block.0.1.weight\", \"backbone.3.block.0.1.bias\", \"backbone.3.block.0.1.running_mean\", \"backbone.3.block.0.1.running_var\", \"backbone.3.block.1.0.weight\", \"backbone.3.block.1.1.weight\", \"backbone.3.block.1.1.bias\", \"backbone.3.block.1.1.running_mean\", \"backbone.3.block.1.1.running_var\", \"backbone.3.block.2.0.weight\", \"backbone.3.block.2.1.weight\", \"backbone.3.block.2.1.bias\", \"backbone.3.block.2.1.running_mean\", \"backbone.3.block.2.1.running_var\", \"backbone.4.block.0.0.weight\", \"backbone.4.block.0.1.weight\", \"backbone.4.block.0.1.bias\", \"backbone.4.block.0.1.running_mean\", \"backbone.4.block.0.1.running_var\", \"backbone.4.block.1.0.weight\", \"backbone.4.block.1.1.weight\", \"backbone.4.block.1.1.bias\", \"backbone.4.block.1.1.running_mean\", \"backbone.4.block.1.1.running_var\", \"backbone.4.block.2.fc1.weight\", \"backbone.4.block.2.fc1.bias\", \"backbone.4.block.2.fc2.weight\", \"backbone.4.block.2.fc2.bias\", \"backbone.4.block.3.0.weight\", \"backbone.4.block.3.1.weight\", \"backbone.4.block.3.1.bias\", \"backbone.4.block.3.1.running_mean\", \"backbone.4.block.3.1.running_var\", \"backbone.5.block.0.0.weight\", \"backbone.5.block.0.1.weight\", \"backbone.5.block.0.1.bias\", \"backbone.5.block.0.1.running_mean\", \"backbone.5.block.0.1.running_var\", \"backbone.5.block.1.0.weight\", \"backbone.5.block.1.1.weight\", \"backbone.5.block.1.1.bias\", \"backbone.5.block.1.1.running_mean\", \"backbone.5.block.1.1.running_var\", \"backbone.5.block.2.fc1.weight\", \"backbone.5.block.2.fc1.bias\", \"backbone.5.block.2.fc2.weight\", \"backbone.5.block.2.fc2.bias\", \"backbone.5.block.3.0.weight\", \"backbone.5.block.3.1.weight\", \"backbone.5.block.3.1.bias\", \"backbone.5.block.3.1.running_mean\", \"backbone.5.block.3.1.running_var\", \"backbone.6.block.0.0.weight\", \"backbone.6.block.0.1.weight\", \"backbone.6.block.0.1.bias\", \"backbone.6.block.0.1.running_mean\", \"backbone.6.block.0.1.running_var\", \"backbone.6.block.1.0.weight\", \"backbone.6.block.1.1.weight\", \"backbone.6.block.1.1.bias\", \"backbone.6.block.1.1.running_mean\", \"backbone.6.block.1.1.running_var\", \"backbone.6.block.2.fc1.weight\", \"backbone.6.block.2.fc1.bias\", \"backbone.6.block.2.fc2.weight\", \"backbone.6.block.2.fc2.bias\", \"backbone.6.block.3.0.weight\", \"backbone.6.block.3.1.weight\", \"backbone.6.block.3.1.bias\", \"backbone.6.block.3.1.running_mean\", \"backbone.6.block.3.1.running_var\", \"backbone.7.block.0.0.weight\", \"backbone.7.block.0.1.weight\", \"backbone.7.block.0.1.bias\", \"backbone.7.block.0.1.running_mean\", \"backbone.7.block.0.1.running_var\", \"backbone.7.block.1.0.weight\", \"backbone.7.block.1.1.weight\", \"backbone.7.block.1.1.bias\", \"backbone.7.block.1.1.running_mean\", \"backbone.7.block.1.1.running_var\", \"backbone.7.block.2.0.weight\", \"backbone.7.block.2.1.weight\", \"backbone.7.block.2.1.bias\", \"backbone.7.block.2.1.running_mean\", \"backbone.7.block.2.1.running_var\", \"backbone.8.block.0.0.weight\", \"backbone.8.block.0.1.weight\", \"backbone.8.block.0.1.bias\", \"backbone.8.block.0.1.running_mean\", \"backbone.8.block.0.1.running_var\", \"backbone.8.block.1.0.weight\", \"backbone.8.block.1.1.weight\", \"backbone.8.block.1.1.bias\", \"backbone.8.block.1.1.running_mean\", \"backbone.8.block.1.1.running_var\", \"backbone.8.block.2.0.weight\", \"backbone.8.block.2.1.weight\", \"backbone.8.block.2.1.bias\", \"backbone.8.block.2.1.running_mean\", \"backbone.8.block.2.1.running_var\", \"backbone.9.block.0.0.weight\", \"backbone.9.block.0.1.weight\", \"backbone.9.block.0.1.bias\", \"backbone.9.block.0.1.running_mean\", \"backbone.9.block.0.1.running_var\", \"backbone.9.block.1.0.weight\", \"backbone.9.block.1.1.weight\", \"backbone.9.block.1.1.bias\", \"backbone.9.block.1.1.running_mean\", \"backbone.9.block.1.1.running_var\", \"backbone.9.block.2.0.weight\", \"backbone.9.block.2.1.weight\", \"backbone.9.block.2.1.bias\", \"backbone.9.block.2.1.running_mean\", \"backbone.9.block.2.1.running_var\", \"backbone.10.block.0.0.weight\", \"backbone.10.block.0.1.weight\", \"backbone.10.block.0.1.bias\", \"backbone.10.block.0.1.running_mean\", \"backbone.10.block.0.1.running_var\", \"backbone.10.block.1.0.weight\", \"backbone.10.block.1.1.weight\", \"backbone.10.block.1.1.bias\", \"backbone.10.block.1.1.running_mean\", \"backbone.10.block.1.1.running_var\", \"backbone.10.block.2.0.weight\", \"backbone.10.block.2.1.weight\", \"backbone.10.block.2.1.bias\", \"backbone.10.block.2.1.running_mean\", \"backbone.10.block.2.1.running_var\", \"backbone.11.block.0.0.weight\", \"backbone.11.block.0.1.weight\", \"backbone.11.block.0.1.bias\", \"backbone.11.block.0.1.running_mean\", \"backbone.11.block.0.1.running_var\", \"backbone.11.block.1.0.weight\", \"backbone.11.block.1.1.weight\", \"backbone.11.block.1.1.bias\", \"backbone.11.block.1.1.running_mean\", \"backbone.11.block.1.1.running_var\", \"backbone.11.block.2.fc1.weight\", \"backbone.11.block.2.fc1.bias\", \"backbone.11.block.2.fc2.weight\", \"backbone.11.block.2.fc2.bias\", \"backbone.11.block.3.0.weight\", \"backbone.11.block.3.1.weight\", \"backbone.11.block.3.1.bias\", \"backbone.11.block.3.1.running_mean\", \"backbone.11.block.3.1.running_var\", \"backbone.12.block.0.0.weight\", \"backbone.12.block.0.1.weight\", \"backbone.12.block.0.1.bias\", \"backbone.12.block.0.1.running_mean\", \"backbone.12.block.0.1.running_var\", \"backbone.12.block.1.0.weight\", \"backbone.12.block.1.1.weight\", \"backbone.12.block.1.1.bias\", \"backbone.12.block.1.1.running_mean\", \"backbone.12.block.1.1.running_var\", \"backbone.12.block.2.fc1.weight\", \"backbone.12.block.2.fc1.bias\", \"backbone.12.block.2.fc2.weight\", \"backbone.12.block.2.fc2.bias\", \"backbone.12.block.3.0.weight\", \"backbone.12.block.3.1.weight\", \"backbone.12.block.3.1.bias\", \"backbone.12.block.3.1.running_mean\", \"backbone.12.block.3.1.running_var\", \"backbone.13.block.0.0.weight\", \"backbone.13.block.0.1.weight\", \"backbone.13.block.0.1.bias\", \"backbone.13.block.0.1.running_mean\", \"backbone.13.block.0.1.running_var\", \"backbone.13.block.1.0.weight\", \"backbone.13.block.1.1.weight\", \"backbone.13.block.1.1.bias\", \"backbone.13.block.1.1.running_mean\", \"backbone.13.block.1.1.running_var\", \"backbone.13.block.2.fc1.weight\", \"backbone.13.block.2.fc1.bias\", \"backbone.13.block.2.fc2.weight\", \"backbone.13.block.2.fc2.bias\", \"backbone.13.block.3.0.weight\", \"backbone.13.block.3.1.weight\", \"backbone.13.block.3.1.bias\", \"backbone.13.block.3.1.running_mean\", \"backbone.13.block.3.1.running_var\", \"backbone.14.block.0.0.weight\", \"backbone.14.block.0.1.weight\", \"backbone.14.block.0.1.bias\", \"backbone.14.block.0.1.running_mean\", \"backbone.14.block.0.1.running_var\", \"backbone.14.block.1.0.weight\", \"backbone.14.block.1.1.weight\", \"backbone.14.block.1.1.bias\", \"backbone.14.block.1.1.running_mean\", \"backbone.14.block.1.1.running_var\", \"backbone.14.block.2.fc1.weight\", \"backbone.14.block.2.fc1.bias\", \"backbone.14.block.2.fc2.weight\", \"backbone.14.block.2.fc2.bias\", \"backbone.14.block.3.0.weight\", \"backbone.14.block.3.1.weight\", \"backbone.14.block.3.1.bias\", \"backbone.14.block.3.1.running_mean\", \"backbone.14.block.3.1.running_var\", \"backbone.15.block.0.0.weight\", \"backbone.15.block.0.1.weight\", \"backbone.15.block.0.1.bias\", \"backbone.15.block.0.1.running_mean\", \"backbone.15.block.0.1.running_var\", \"backbone.15.block.1.0.weight\", \"backbone.15.block.1.1.weight\", \"backbone.15.block.1.1.bias\", \"backbone.15.block.1.1.running_mean\", \"backbone.15.block.1.1.running_var\", \"backbone.15.block.2.fc1.weight\", \"backbone.15.block.2.fc1.bias\", \"backbone.15.block.2.fc2.weight\", \"backbone.15.block.2.fc2.bias\", \"backbone.15.block.3.0.weight\", \"backbone.15.block.3.1.weight\", \"backbone.15.block.3.1.bias\", \"backbone.15.block.3.1.running_mean\", \"backbone.15.block.3.1.running_var\", \"backbone.16.0.weight\", \"backbone.16.1.weight\", \"backbone.16.1.bias\", \"backbone.16.1.running_mean\", \"backbone.16.1.running_var\", \"classifier.0.convs.0.0.weight\", \"classifier.0.convs.0.1.weight\", \"classifier.0.convs.0.1.bias\", \"classifier.0.convs.0.1.running_mean\", \"classifier.0.convs.0.1.running_var\", \"classifier.0.convs.1.0.weight\", \"classifier.0.convs.1.1.weight\", \"classifier.0.convs.1.1.bias\", \"classifier.0.convs.1.1.running_mean\", \"classifier.0.convs.1.1.running_var\", \"classifier.0.convs.2.0.weight\", \"classifier.0.convs.2.1.weight\", \"classifier.0.convs.2.1.bias\", \"classifier.0.convs.2.1.running_mean\", \"classifier.0.convs.2.1.running_var\", \"classifier.0.convs.3.0.weight\", \"classifier.0.convs.3.1.weight\", \"classifier.0.convs.3.1.bias\", \"classifier.0.convs.3.1.running_mean\", \"classifier.0.convs.3.1.running_var\", \"classifier.0.convs.4.1.weight\", \"classifier.0.convs.4.2.weight\", \"classifier.0.convs.4.2.bias\", \"classifier.0.convs.4.2.running_mean\", \"classifier.0.convs.4.2.running_var\", \"classifier.0.project.0.weight\", \"classifier.0.project.1.weight\", \"classifier.0.project.1.bias\", \"classifier.0.project.1.running_mean\", \"classifier.0.project.1.running_var\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.2.running_mean\", \"classifier.2.running_var\". \n\tUnexpected key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.bn1.num_batches_tracked\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.bn1.num_batches_tracked\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.bn2.num_batches_tracked\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.bn3.num_batches_tracked\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.0.downsample.1.num_batches_tracked\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.bn1.num_batches_tracked\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.bn2.num_batches_tracked\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.1.bn3.num_batches_tracked\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.bn1.num_batches_tracked\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.bn2.num_batches_tracked\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer1.2.bn3.num_batches_tracked\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.bn1.num_batches_tracked\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.bn2.num_batches_tracked\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.bn3.num_batches_tracked\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.0.downsample.1.num_batches_tracked\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.bn1.num_batches_tracked\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.bn2.num_batches_tracked\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.1.bn3.num_batches_tracked\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.bn1.num_batches_tracked\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.bn2.num_batches_tracked\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.2.bn3.num_batches_tracked\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.bn1.num_batches_tracked\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.bn2.num_batches_tracked\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer2.3.bn3.num_batches_tracked\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.bn1.num_batches_tracked\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.bn2.num_batches_tracked\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.bn3.num_batches_tracked\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.0.downsample.1.num_batches_tracked\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.bn1.num_batches_tracked\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.bn2.num_batches_tracked\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.1.bn3.num_batches_tracked\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.bn1.num_batches_tracked\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.bn2.num_batches_tracked\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.2.bn3.num_batches_tracked\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.bn1.num_batches_tracked\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.bn2.num_batches_tracked\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.3.bn3.num_batches_tracked\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.bn1.num_batches_tracked\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.bn2.num_batches_tracked\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.4.bn3.num_batches_tracked\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.bn1.num_batches_tracked\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.bn2.num_batches_tracked\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer3.5.bn3.num_batches_tracked\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.bn1.num_batches_tracked\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.bn2.num_batches_tracked\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.bn3.num_batches_tracked\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.0.downsample.1.num_batches_tracked\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.bn1.num_batches_tracked\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.bn2.num_batches_tracked\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.1.bn3.num_batches_tracked\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.bn1.num_batches_tracked\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.bn2.num_batches_tracked\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"backbone.layer4.2.bn3.num_batches_tracked\", \"classifier.6.weight\", \"classifier.6.bias\", \"classifier.0.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.4.running_mean\", \"classifier.4.running_var\", \"classifier.4.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([25, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([25]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 모델 정의 및 저장된 가중치 불러오기\u001b[39;00m\n\u001b[1;32m     98\u001b[0m model \u001b[38;5;241m=\u001b[39m deeplabv3_mobilenet_v3_large(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_autonomous_driving_segmentation_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 일부 레이어만 학습 가능하도록 설정\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"backbone.0.0.weight\", \"backbone.0.1.weight\", \"backbone.0.1.bias\", \"backbone.0.1.running_mean\", \"backbone.0.1.running_var\", \"backbone.1.block.0.0.weight\", \"backbone.1.block.0.1.weight\", \"backbone.1.block.0.1.bias\", \"backbone.1.block.0.1.running_mean\", \"backbone.1.block.0.1.running_var\", \"backbone.1.block.1.0.weight\", \"backbone.1.block.1.1.weight\", \"backbone.1.block.1.1.bias\", \"backbone.1.block.1.1.running_mean\", \"backbone.1.block.1.1.running_var\", \"backbone.2.block.0.0.weight\", \"backbone.2.block.0.1.weight\", \"backbone.2.block.0.1.bias\", \"backbone.2.block.0.1.running_mean\", \"backbone.2.block.0.1.running_var\", \"backbone.2.block.1.0.weight\", \"backbone.2.block.1.1.weight\", \"backbone.2.block.1.1.bias\", \"backbone.2.block.1.1.running_mean\", \"backbone.2.block.1.1.running_var\", \"backbone.2.block.2.0.weight\", \"backbone.2.block.2.1.weight\", \"backbone.2.block.2.1.bias\", \"backbone.2.block.2.1.running_mean\", \"backbone.2.block.2.1.running_var\", \"backbone.3.block.0.0.weight\", \"backbone.3.block.0.1.weight\", \"backbone.3.block.0.1.bias\", \"backbone.3.block.0.1.running_mean\", \"backbone.3.block.0.1.running_var\", \"backbone.3.block.1.0.weight\", \"backbone.3.block.1.1.weight\", \"backbone.3.block.1.1.bias\", \"backbone.3.block.1.1.running_mean\", \"backbone.3.block.1.1.running_var\", \"backbone.3.block.2.0.weight\", \"backbone.3.block.2.1.weight\", \"backbone.3.block.2.1.bias\", \"backbone.3.block.2.1.running_mean\", \"backbone.3.block.2.1.running_var\", \"backbone.4.block.0.0.weight\", \"backbone.4.block.0.1.weight\", \"backbone.4.block.0.1.bias\", \"backbone.4.block.0.1.running_mean\", \"backbone.4.block.0.1.running_var\", \"backbone.4.block.1.0.weight\", \"backbone.4.block.1.1.weight\", \"backbone.4.block.1.1.bias\", \"backbone.4.block.1.1.running_mean\", \"backbone.4.block.1.1.running_var\", \"backbone.4.block.2.fc1.weight\", \"backbone.4.block.2.fc1.bias\", \"backbone.4.block.2.fc2.weight\", \"backbone.4.block.2.fc2.bias\", \"backbone.4.block.3.0.weight\", \"backbone.4.block.3.1.weight\", \"backbone.4.block.3.1.bias\", \"backbone.4.block.3.1.running_mean\", \"backbone.4.block.3.1.running_var\", \"backbone.5.block.0.0.weight\", \"backbone.5.block.0.1.weight\", \"backbone.5.block.0.1.bias\", \"backbone.5.block.0.1.running_mean\", \"backbone.5.block.0.1.running_var\", \"backbone.5.block.1.0.weight\", \"backbone.5.block.1.1.weight\", \"backbone.5.block.1.1.bias\", \"backbone.5.block.1.1.running_mean\", \"backbone.5.block.1.1.running_var\", \"backbone.5.block.2.fc1.weight\", \"backbone.5.block.2.fc1.bias\", \"backbone.5.block.2.fc2.weight\", \"backbone.5.block.2.fc2.bias\", \"backbone.5.block.3.0.weight\", \"backbone.5.block.3.1.weight\", \"backbone.5.block.3.1.bias\", \"backbone.5.block.3.1.running_mean\", \"backbone.5.block.3.1.running_var\", \"backbone.6.block.0.0.weight\", \"backbone.6.block.0.1.weight\", \"backbone.6.block.0.1.bias\", \"backbone.6.block.0.1.running_mean\", \"backbone.6.block.0.1.running_var\", \"backbone.6.block.1.0.weight\", \"backbone.6.block.1.1.weight\", \"backbone.6.block.1.1.bias\", \"backbone.6.block.1.1.running_mean\", \"backbone.6.block.1.1.running_var\", \"backbone.6.block.2.fc1.weight\", \"backbone.6.block.2.fc1.bias\", \"backbone.6.block.2.fc2.weight\", \"backbone.6.block.2.fc2.bias\", \"backbone.6.block.3.0.weight\", \"backbone.6.block.3.1.weight\", \"backbone.6.block.3.1.bias\", \"backbone.6.block.3.1.running_mean\", \"backbone.6.block.3.1.running_var\", \"backbone.7.block.0.0.weight\", \"backbone.7.block.0.1.weight\", \"backbone.7.block.0.1.bias\", \"backbone.7.block.0.1.running_mean\", \"backbone.7.block.0.1.running_var\", \"backbone.7.block.1.0.weight\", \"backbone.7.block.1.1.weight\", \"backbone.7.block.1.1.bias\", \"backbone.7.block.1.1.running_mean\", \"backbone.7.block.1.1.running_var\", \"backbone.7.block.2.0.weight\", \"backbone.7.block.2.1.weight\", \"backbone.7.block.2.1.bias\", \"backbone.7.block.2.1.running_mean\", \"backbone.7.block.2.1.running_var\", \"backbone.8.block.0.0.weight\", \"backbone.8.block.0.1.weight\", \"backbone.8.block.0.1.bias\", \"backbone.8.block.0.1.running_mean\", \"backbone.8.block.0.1.running_var\", \"backbone.8.block.1.0.weight\", \"backbone.8.block.1.1.weight\", \"backbone.8.block.1.1.bias\", \"backbone.8.block.1.1.running_mean\", \"backbone.8.block.1.1.running_var\", \"backbone.8.block.2.0.weight\", \"backbone.8.block.2.1.weight\", \"backbone.8.block.2.1.bias\", \"backbone.8.block.2.1.running_mean\", \"backbone.8.block.2.1.running_var\", \"backbone.9.block.0.0.weight\", \"backbone.9.block.0.1.weight\", \"backbone.9.block.0.1.bias\", \"backbone.9.block.0.1.running_mean\", \"backbone.9.block.0.1.running_var\", \"backbone.9.block.1.0.weight\", \"backbone.9.block.1.1.weight\", \"backbone.9.block.1.1.bias\", \"backbone.9.block.1.1.running_mean\", \"backbone.9.block.1.1.running_var\", \"backbone.9.block.2.0.weight\", \"backbone.9.block.2.1.weight\", \"backbone.9.block.2.1.bias\", \"backbone.9.block.2.1.running_mean\", \"backbone.9.block.2.1.running_var\", \"backbone.10.block.0.0.weight\", \"backbone.10.block.0.1.weight\", \"backbone.10.block.0.1.bias\", \"backbone.10.block.0.1.running_mean\", \"backbone.10.block.0.1.running_var\", \"backbone.10.block.1.0.weight\", \"backbone.10.block.1.1.weight\", \"backbone.10.block.1.1.bias\", \"backbone.10.block.1.1.running_mean\", \"backbone.10.block.1.1.running_var\", \"backbone.10.block.2.0.weight\", \"backbone.10.block.2.1.weight\", \"backbone.10.block.2.1.bias\", \"backbone.10.block.2.1.running_mean\", \"backbone.10.block.2.1.running_var\", \"backbone.11.block.0.0.weight\", \"backbone.11.block.0.1.weight\", \"backbone.11.block.0.1.bias\", \"backbone.11.block.0.1.running_mean\", \"backbone.11.block.0.1.running_var\", \"backbone.11.block.1.0.weight\", \"backbone.11.block.1.1.weight\", \"backbone.11.block.1.1.bias\", \"backbone.11.block.1.1.running_mean\", \"backbone.11.block.1.1.running_var\", \"backbone.11.block.2.fc1.weight\", \"backbone.11.block.2.fc1.bias\", \"backbone.11.block.2.fc2.weight\", \"backbone.11.block.2.fc2.bias\", \"backbone.11.block.3.0.weight\", \"backbone.11.block.3.1.weight\", \"backbone.11.block.3.1.bias\", \"backbone.11.block.3.1.running_mean\", \"backbone.11.block.3.1.running_var\", \"backbone.12.block.0.0.weight\", \"backbone.12.block.0.1.weight\", \"backbone.12.block.0.1.bias\", \"backbone.12.block.0.1.running_mean\", \"backbone.12.block.0.1.running_var\", \"backbone.12.block.1.0.weight\", \"backbone.12.block.1.1.weight\", \"backbone.12.block.1.1.bias\", \"backbone.12.block.1.1.running_mean\", \"backbone.12.block.1.1.running_var\", \"backbone.12.block.2.fc1.weight\", \"backbone.12.block.2.fc1.bias\", \"backbone.12.block.2.fc2.weight\", \"backbone.12.block.2.fc2.bias\", \"backbone.12.block.3.0.weight\", \"backbone.12.block.3.1.weight\", \"backbone.12.block.3.1.bias\", \"backbone.12.block.3.1.running_mean\", \"backbone.12.block.3.1.running_var\", \"backbone.13.block.0.0.weight\", \"backbone.13.block.0.1.weight\", \"backbone.13.block.0.1.bias\", \"backbone.13.block.0.1.running_mean\", \"backbone.13.block.0.1.running_var\", \"backbone.13.block.1.0.weight\", \"backbone.13.block.1.1.weight\", \"backbone.13.block.1.1.bias\", \"backbone.13.block.1.1.running_mean\", \"backbone.13.block.1.1.running_var\", \"backbone.13.block.2.fc1.weight\", \"backbone.13.block.2.fc1.bias\", \"backbone.13.block.2.fc2.weight\", \"backbone.13.block.2.fc2.bias\", \"backbone.13.block.3.0.weight\", \"backbone.13.block.3.1.weight\", \"backbone.13.block.3.1.bias\", \"backbone.13.block.3.1.running_mean\", \"backbone.13.block.3.1.running_var\", \"backbone.14.block.0.0.weight\", \"backbone.14.block.0.1.weight\", \"backbone.14.block.0.1.bias\", \"backbone.14.block.0.1.running_mean\", \"backbone.14.block.0.1.running_var\", \"backbone.14.block.1.0.weight\", \"backbone.14.block.1.1.weight\", \"backbone.14.block.1.1.bias\", \"backbone.14.block.1.1.running_mean\", \"backbone.14.block.1.1.running_var\", \"backbone.14.block.2.fc1.weight\", \"backbone.14.block.2.fc1.bias\", \"backbone.14.block.2.fc2.weight\", \"backbone.14.block.2.fc2.bias\", \"backbone.14.block.3.0.weight\", \"backbone.14.block.3.1.weight\", \"backbone.14.block.3.1.bias\", \"backbone.14.block.3.1.running_mean\", \"backbone.14.block.3.1.running_var\", \"backbone.15.block.0.0.weight\", \"backbone.15.block.0.1.weight\", \"backbone.15.block.0.1.bias\", \"backbone.15.block.0.1.running_mean\", \"backbone.15.block.0.1.running_var\", \"backbone.15.block.1.0.weight\", \"backbone.15.block.1.1.weight\", \"backbone.15.block.1.1.bias\", \"backbone.15.block.1.1.running_mean\", \"backbone.15.block.1.1.running_var\", \"backbone.15.block.2.fc1.weight\", \"backbone.15.block.2.fc1.bias\", \"backbone.15.block.2.fc2.weight\", \"backbone.15.block.2.fc2.bias\", \"backbone.15.block.3.0.weight\", \"backbone.15.block.3.1.weight\", \"backbone.15.block.3.1.bias\", \"backbone.15.block.3.1.running_mean\", \"backbone.15.block.3.1.running_var\", \"backbone.16.0.weight\", \"backbone.16.1.weight\", \"backbone.16.1.bias\", \"backbone.16.1.running_mean\", \"backbone.16.1.running_var\", \"classifier.0.convs.0.0.weight\", \"classifier.0.convs.0.1.weight\", \"classifier.0.convs.0.1.bias\", \"classifier.0.convs.0.1.running_mean\", \"classifier.0.convs.0.1.running_var\", \"classifier.0.convs.1.0.weight\", \"classifier.0.convs.1.1.weight\", \"classifier.0.convs.1.1.bias\", \"classifier.0.convs.1.1.running_mean\", \"classifier.0.convs.1.1.running_var\", \"classifier.0.convs.2.0.weight\", \"classifier.0.convs.2.1.weight\", \"classifier.0.convs.2.1.bias\", \"classifier.0.convs.2.1.running_mean\", \"classifier.0.convs.2.1.running_var\", \"classifier.0.convs.3.0.weight\", \"classifier.0.convs.3.1.weight\", \"classifier.0.convs.3.1.bias\", \"classifier.0.convs.3.1.running_mean\", \"classifier.0.convs.3.1.running_var\", \"classifier.0.convs.4.1.weight\", \"classifier.0.convs.4.2.weight\", \"classifier.0.convs.4.2.bias\", \"classifier.0.convs.4.2.running_mean\", \"classifier.0.convs.4.2.running_var\", \"classifier.0.project.0.weight\", \"classifier.0.project.1.weight\", \"classifier.0.project.1.bias\", \"classifier.0.project.1.running_mean\", \"classifier.0.project.1.running_var\", \"classifier.2.weight\", \"classifier.2.bias\", \"classifier.2.running_mean\", \"classifier.2.running_var\". \n\tUnexpected key(s) in state_dict: \"backbone.conv1.weight\", \"backbone.bn1.weight\", \"backbone.bn1.bias\", \"backbone.bn1.running_mean\", \"backbone.bn1.running_var\", \"backbone.bn1.num_batches_tracked\", \"backbone.layer1.0.conv1.weight\", \"backbone.layer1.0.bn1.weight\", \"backbone.layer1.0.bn1.bias\", \"backbone.layer1.0.bn1.running_mean\", \"backbone.layer1.0.bn1.running_var\", \"backbone.layer1.0.bn1.num_batches_tracked\", \"backbone.layer1.0.conv2.weight\", \"backbone.layer1.0.bn2.weight\", \"backbone.layer1.0.bn2.bias\", \"backbone.layer1.0.bn2.running_mean\", \"backbone.layer1.0.bn2.running_var\", \"backbone.layer1.0.bn2.num_batches_tracked\", \"backbone.layer1.0.conv3.weight\", \"backbone.layer1.0.bn3.weight\", \"backbone.layer1.0.bn3.bias\", \"backbone.layer1.0.bn3.running_mean\", \"backbone.layer1.0.bn3.running_var\", \"backbone.layer1.0.bn3.num_batches_tracked\", \"backbone.layer1.0.downsample.0.weight\", \"backbone.layer1.0.downsample.1.weight\", \"backbone.layer1.0.downsample.1.bias\", \"backbone.layer1.0.downsample.1.running_mean\", \"backbone.layer1.0.downsample.1.running_var\", \"backbone.layer1.0.downsample.1.num_batches_tracked\", \"backbone.layer1.1.conv1.weight\", \"backbone.layer1.1.bn1.weight\", \"backbone.layer1.1.bn1.bias\", \"backbone.layer1.1.bn1.running_mean\", \"backbone.layer1.1.bn1.running_var\", \"backbone.layer1.1.bn1.num_batches_tracked\", \"backbone.layer1.1.conv2.weight\", \"backbone.layer1.1.bn2.weight\", \"backbone.layer1.1.bn2.bias\", \"backbone.layer1.1.bn2.running_mean\", \"backbone.layer1.1.bn2.running_var\", \"backbone.layer1.1.bn2.num_batches_tracked\", \"backbone.layer1.1.conv3.weight\", \"backbone.layer1.1.bn3.weight\", \"backbone.layer1.1.bn3.bias\", \"backbone.layer1.1.bn3.running_mean\", \"backbone.layer1.1.bn3.running_var\", \"backbone.layer1.1.bn3.num_batches_tracked\", \"backbone.layer1.2.conv1.weight\", \"backbone.layer1.2.bn1.weight\", \"backbone.layer1.2.bn1.bias\", \"backbone.layer1.2.bn1.running_mean\", \"backbone.layer1.2.bn1.running_var\", \"backbone.layer1.2.bn1.num_batches_tracked\", \"backbone.layer1.2.conv2.weight\", \"backbone.layer1.2.bn2.weight\", \"backbone.layer1.2.bn2.bias\", \"backbone.layer1.2.bn2.running_mean\", \"backbone.layer1.2.bn2.running_var\", \"backbone.layer1.2.bn2.num_batches_tracked\", \"backbone.layer1.2.conv3.weight\", \"backbone.layer1.2.bn3.weight\", \"backbone.layer1.2.bn3.bias\", \"backbone.layer1.2.bn3.running_mean\", \"backbone.layer1.2.bn3.running_var\", \"backbone.layer1.2.bn3.num_batches_tracked\", \"backbone.layer2.0.conv1.weight\", \"backbone.layer2.0.bn1.weight\", \"backbone.layer2.0.bn1.bias\", \"backbone.layer2.0.bn1.running_mean\", \"backbone.layer2.0.bn1.running_var\", \"backbone.layer2.0.bn1.num_batches_tracked\", \"backbone.layer2.0.conv2.weight\", \"backbone.layer2.0.bn2.weight\", \"backbone.layer2.0.bn2.bias\", \"backbone.layer2.0.bn2.running_mean\", \"backbone.layer2.0.bn2.running_var\", \"backbone.layer2.0.bn2.num_batches_tracked\", \"backbone.layer2.0.conv3.weight\", \"backbone.layer2.0.bn3.weight\", \"backbone.layer2.0.bn3.bias\", \"backbone.layer2.0.bn3.running_mean\", \"backbone.layer2.0.bn3.running_var\", \"backbone.layer2.0.bn3.num_batches_tracked\", \"backbone.layer2.0.downsample.0.weight\", \"backbone.layer2.0.downsample.1.weight\", \"backbone.layer2.0.downsample.1.bias\", \"backbone.layer2.0.downsample.1.running_mean\", \"backbone.layer2.0.downsample.1.running_var\", \"backbone.layer2.0.downsample.1.num_batches_tracked\", \"backbone.layer2.1.conv1.weight\", \"backbone.layer2.1.bn1.weight\", \"backbone.layer2.1.bn1.bias\", \"backbone.layer2.1.bn1.running_mean\", \"backbone.layer2.1.bn1.running_var\", \"backbone.layer2.1.bn1.num_batches_tracked\", \"backbone.layer2.1.conv2.weight\", \"backbone.layer2.1.bn2.weight\", \"backbone.layer2.1.bn2.bias\", \"backbone.layer2.1.bn2.running_mean\", \"backbone.layer2.1.bn2.running_var\", \"backbone.layer2.1.bn2.num_batches_tracked\", \"backbone.layer2.1.conv3.weight\", \"backbone.layer2.1.bn3.weight\", \"backbone.layer2.1.bn3.bias\", \"backbone.layer2.1.bn3.running_mean\", \"backbone.layer2.1.bn3.running_var\", \"backbone.layer2.1.bn3.num_batches_tracked\", \"backbone.layer2.2.conv1.weight\", \"backbone.layer2.2.bn1.weight\", \"backbone.layer2.2.bn1.bias\", \"backbone.layer2.2.bn1.running_mean\", \"backbone.layer2.2.bn1.running_var\", \"backbone.layer2.2.bn1.num_batches_tracked\", \"backbone.layer2.2.conv2.weight\", \"backbone.layer2.2.bn2.weight\", \"backbone.layer2.2.bn2.bias\", \"backbone.layer2.2.bn2.running_mean\", \"backbone.layer2.2.bn2.running_var\", \"backbone.layer2.2.bn2.num_batches_tracked\", \"backbone.layer2.2.conv3.weight\", \"backbone.layer2.2.bn3.weight\", \"backbone.layer2.2.bn3.bias\", \"backbone.layer2.2.bn3.running_mean\", \"backbone.layer2.2.bn3.running_var\", \"backbone.layer2.2.bn3.num_batches_tracked\", \"backbone.layer2.3.conv1.weight\", \"backbone.layer2.3.bn1.weight\", \"backbone.layer2.3.bn1.bias\", \"backbone.layer2.3.bn1.running_mean\", \"backbone.layer2.3.bn1.running_var\", \"backbone.layer2.3.bn1.num_batches_tracked\", \"backbone.layer2.3.conv2.weight\", \"backbone.layer2.3.bn2.weight\", \"backbone.layer2.3.bn2.bias\", \"backbone.layer2.3.bn2.running_mean\", \"backbone.layer2.3.bn2.running_var\", \"backbone.layer2.3.bn2.num_batches_tracked\", \"backbone.layer2.3.conv3.weight\", \"backbone.layer2.3.bn3.weight\", \"backbone.layer2.3.bn3.bias\", \"backbone.layer2.3.bn3.running_mean\", \"backbone.layer2.3.bn3.running_var\", \"backbone.layer2.3.bn3.num_batches_tracked\", \"backbone.layer3.0.conv1.weight\", \"backbone.layer3.0.bn1.weight\", \"backbone.layer3.0.bn1.bias\", \"backbone.layer3.0.bn1.running_mean\", \"backbone.layer3.0.bn1.running_var\", \"backbone.layer3.0.bn1.num_batches_tracked\", \"backbone.layer3.0.conv2.weight\", \"backbone.layer3.0.bn2.weight\", \"backbone.layer3.0.bn2.bias\", \"backbone.layer3.0.bn2.running_mean\", \"backbone.layer3.0.bn2.running_var\", \"backbone.layer3.0.bn2.num_batches_tracked\", \"backbone.layer3.0.conv3.weight\", \"backbone.layer3.0.bn3.weight\", \"backbone.layer3.0.bn3.bias\", \"backbone.layer3.0.bn3.running_mean\", \"backbone.layer3.0.bn3.running_var\", \"backbone.layer3.0.bn3.num_batches_tracked\", \"backbone.layer3.0.downsample.0.weight\", \"backbone.layer3.0.downsample.1.weight\", \"backbone.layer3.0.downsample.1.bias\", \"backbone.layer3.0.downsample.1.running_mean\", \"backbone.layer3.0.downsample.1.running_var\", \"backbone.layer3.0.downsample.1.num_batches_tracked\", \"backbone.layer3.1.conv1.weight\", \"backbone.layer3.1.bn1.weight\", \"backbone.layer3.1.bn1.bias\", \"backbone.layer3.1.bn1.running_mean\", \"backbone.layer3.1.bn1.running_var\", \"backbone.layer3.1.bn1.num_batches_tracked\", \"backbone.layer3.1.conv2.weight\", \"backbone.layer3.1.bn2.weight\", \"backbone.layer3.1.bn2.bias\", \"backbone.layer3.1.bn2.running_mean\", \"backbone.layer3.1.bn2.running_var\", \"backbone.layer3.1.bn2.num_batches_tracked\", \"backbone.layer3.1.conv3.weight\", \"backbone.layer3.1.bn3.weight\", \"backbone.layer3.1.bn3.bias\", \"backbone.layer3.1.bn3.running_mean\", \"backbone.layer3.1.bn3.running_var\", \"backbone.layer3.1.bn3.num_batches_tracked\", \"backbone.layer3.2.conv1.weight\", \"backbone.layer3.2.bn1.weight\", \"backbone.layer3.2.bn1.bias\", \"backbone.layer3.2.bn1.running_mean\", \"backbone.layer3.2.bn1.running_var\", \"backbone.layer3.2.bn1.num_batches_tracked\", \"backbone.layer3.2.conv2.weight\", \"backbone.layer3.2.bn2.weight\", \"backbone.layer3.2.bn2.bias\", \"backbone.layer3.2.bn2.running_mean\", \"backbone.layer3.2.bn2.running_var\", \"backbone.layer3.2.bn2.num_batches_tracked\", \"backbone.layer3.2.conv3.weight\", \"backbone.layer3.2.bn3.weight\", \"backbone.layer3.2.bn3.bias\", \"backbone.layer3.2.bn3.running_mean\", \"backbone.layer3.2.bn3.running_var\", \"backbone.layer3.2.bn3.num_batches_tracked\", \"backbone.layer3.3.conv1.weight\", \"backbone.layer3.3.bn1.weight\", \"backbone.layer3.3.bn1.bias\", \"backbone.layer3.3.bn1.running_mean\", \"backbone.layer3.3.bn1.running_var\", \"backbone.layer3.3.bn1.num_batches_tracked\", \"backbone.layer3.3.conv2.weight\", \"backbone.layer3.3.bn2.weight\", \"backbone.layer3.3.bn2.bias\", \"backbone.layer3.3.bn2.running_mean\", \"backbone.layer3.3.bn2.running_var\", \"backbone.layer3.3.bn2.num_batches_tracked\", \"backbone.layer3.3.conv3.weight\", \"backbone.layer3.3.bn3.weight\", \"backbone.layer3.3.bn3.bias\", \"backbone.layer3.3.bn3.running_mean\", \"backbone.layer3.3.bn3.running_var\", \"backbone.layer3.3.bn3.num_batches_tracked\", \"backbone.layer3.4.conv1.weight\", \"backbone.layer3.4.bn1.weight\", \"backbone.layer3.4.bn1.bias\", \"backbone.layer3.4.bn1.running_mean\", \"backbone.layer3.4.bn1.running_var\", \"backbone.layer3.4.bn1.num_batches_tracked\", \"backbone.layer3.4.conv2.weight\", \"backbone.layer3.4.bn2.weight\", \"backbone.layer3.4.bn2.bias\", \"backbone.layer3.4.bn2.running_mean\", \"backbone.layer3.4.bn2.running_var\", \"backbone.layer3.4.bn2.num_batches_tracked\", \"backbone.layer3.4.conv3.weight\", \"backbone.layer3.4.bn3.weight\", \"backbone.layer3.4.bn3.bias\", \"backbone.layer3.4.bn3.running_mean\", \"backbone.layer3.4.bn3.running_var\", \"backbone.layer3.4.bn3.num_batches_tracked\", \"backbone.layer3.5.conv1.weight\", \"backbone.layer3.5.bn1.weight\", \"backbone.layer3.5.bn1.bias\", \"backbone.layer3.5.bn1.running_mean\", \"backbone.layer3.5.bn1.running_var\", \"backbone.layer3.5.bn1.num_batches_tracked\", \"backbone.layer3.5.conv2.weight\", \"backbone.layer3.5.bn2.weight\", \"backbone.layer3.5.bn2.bias\", \"backbone.layer3.5.bn2.running_mean\", \"backbone.layer3.5.bn2.running_var\", \"backbone.layer3.5.bn2.num_batches_tracked\", \"backbone.layer3.5.conv3.weight\", \"backbone.layer3.5.bn3.weight\", \"backbone.layer3.5.bn3.bias\", \"backbone.layer3.5.bn3.running_mean\", \"backbone.layer3.5.bn3.running_var\", \"backbone.layer3.5.bn3.num_batches_tracked\", \"backbone.layer4.0.conv1.weight\", \"backbone.layer4.0.bn1.weight\", \"backbone.layer4.0.bn1.bias\", \"backbone.layer4.0.bn1.running_mean\", \"backbone.layer4.0.bn1.running_var\", \"backbone.layer4.0.bn1.num_batches_tracked\", \"backbone.layer4.0.conv2.weight\", \"backbone.layer4.0.bn2.weight\", \"backbone.layer4.0.bn2.bias\", \"backbone.layer4.0.bn2.running_mean\", \"backbone.layer4.0.bn2.running_var\", \"backbone.layer4.0.bn2.num_batches_tracked\", \"backbone.layer4.0.conv3.weight\", \"backbone.layer4.0.bn3.weight\", \"backbone.layer4.0.bn3.bias\", \"backbone.layer4.0.bn3.running_mean\", \"backbone.layer4.0.bn3.running_var\", \"backbone.layer4.0.bn3.num_batches_tracked\", \"backbone.layer4.0.downsample.0.weight\", \"backbone.layer4.0.downsample.1.weight\", \"backbone.layer4.0.downsample.1.bias\", \"backbone.layer4.0.downsample.1.running_mean\", \"backbone.layer4.0.downsample.1.running_var\", \"backbone.layer4.0.downsample.1.num_batches_tracked\", \"backbone.layer4.1.conv1.weight\", \"backbone.layer4.1.bn1.weight\", \"backbone.layer4.1.bn1.bias\", \"backbone.layer4.1.bn1.running_mean\", \"backbone.layer4.1.bn1.running_var\", \"backbone.layer4.1.bn1.num_batches_tracked\", \"backbone.layer4.1.conv2.weight\", \"backbone.layer4.1.bn2.weight\", \"backbone.layer4.1.bn2.bias\", \"backbone.layer4.1.bn2.running_mean\", \"backbone.layer4.1.bn2.running_var\", \"backbone.layer4.1.bn2.num_batches_tracked\", \"backbone.layer4.1.conv3.weight\", \"backbone.layer4.1.bn3.weight\", \"backbone.layer4.1.bn3.bias\", \"backbone.layer4.1.bn3.running_mean\", \"backbone.layer4.1.bn3.running_var\", \"backbone.layer4.1.bn3.num_batches_tracked\", \"backbone.layer4.2.conv1.weight\", \"backbone.layer4.2.bn1.weight\", \"backbone.layer4.2.bn1.bias\", \"backbone.layer4.2.bn1.running_mean\", \"backbone.layer4.2.bn1.running_var\", \"backbone.layer4.2.bn1.num_batches_tracked\", \"backbone.layer4.2.conv2.weight\", \"backbone.layer4.2.bn2.weight\", \"backbone.layer4.2.bn2.bias\", \"backbone.layer4.2.bn2.running_mean\", \"backbone.layer4.2.bn2.running_var\", \"backbone.layer4.2.bn2.num_batches_tracked\", \"backbone.layer4.2.conv3.weight\", \"backbone.layer4.2.bn3.weight\", \"backbone.layer4.2.bn3.bias\", \"backbone.layer4.2.bn3.running_mean\", \"backbone.layer4.2.bn3.running_var\", \"backbone.layer4.2.bn3.num_batches_tracked\", \"classifier.6.weight\", \"classifier.6.bias\", \"classifier.0.weight\", \"classifier.1.bias\", \"classifier.1.running_mean\", \"classifier.1.running_var\", \"classifier.1.num_batches_tracked\", \"classifier.3.weight\", \"classifier.4.running_mean\", \"classifier.4.running_var\", \"classifier.4.num_batches_tracked\". \n\tsize mismatch for classifier.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for classifier.4.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([25, 256, 1, 1]).\n\tsize mismatch for classifier.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([25])."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, original_transform=None, augment_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.original_transform = original_transform\n",
    "        self.augment_transform = augment_transform\n",
    "        self.images_dir = os.path.join(root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = sorted(os.listdir(self.images_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) * 2  # 원본 + 증강 데이터\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx // 2\n",
    "        is_augmented = idx % 2 == 1\n",
    "\n",
    "        img_name = self.image_files[real_idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        label_path = os.path.join(self.labels_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        annotations = sorted(label_data['Annotation'], key=lambda x: label_data['Annotation'].index(x), reverse=True)\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        class_mapping = {\n",
    "            'road': 1, 'sidewalk': 2, 'road roughness': 3, 'road boundaries': 4, 'crosswalks': 5,\n",
    "            'lane': 6, 'road color guide': 7, 'road marking': 8, 'parking': 9, 'traffic sign': 10,\n",
    "            'traffic light': 11, 'pole/structural object': 12, 'building': 13, 'tunnel': 14,\n",
    "            'bridge': 15, 'pedestrian': 16, 'vehicle': 17, 'bicycle': 18, 'motorcycle': 19,\n",
    "            'personal mobility': 20, 'dynamic': 21, 'vegetation': 22, 'sky': 23, 'static': 24\n",
    "        }\n",
    "\n",
    "        for annotation in annotations:\n",
    "            points = np.array(annotation['data'][0]).reshape((-1, 1, 2)).astype(np.int32)\n",
    "            class_name = annotation['class_name']\n",
    "            class_id = class_mapping.get(class_name, 0)\n",
    "            cv2.fillPoly(mask, [points], class_id)\n",
    "\n",
    "        if is_augmented and self.augment_transform:\n",
    "            augmented = self.augment_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        elif self.original_transform:\n",
    "            augmented = self.original_transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask.long()\n",
    "\n",
    "# 데이터 증강 정의\n",
    "train_transform = A.Compose([\n",
    "    A.RandomRotate90(),\n",
    "    A.Flip(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.8, alpha_coef=0.1, p=0.5),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), p=0.5),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1), p=0.5),\n",
    "    ], p=0.3),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 원본 데이터를 위한 transform\n",
    "original_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset(root_dir='training', original_transform=original_transform, augment_transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# 모델 정의 및 저장된 가중치 불러오기\n",
    "model = deeplabv3_mobilenet_v3_large(weights=None, num_classes=25)\n",
    "model.load_state_dict(torch.load('best_autonomous_driving_segmentation_model.pth'))\n",
    "# \n",
    "# 일부 레이어만 학습 가능하도록 설정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# classifier 레이어만 학습하도록 설정\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의 (학습률 낮게 설정)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 검증 함수\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Early stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 추가 학습 실행\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # 추가 학습할 에폭 수\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, train_loader, criterion, device)  # 실제로는 별도의 검증 데이터셋을 사용해야 합니다\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 최상의 모델 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'finetuned_autonomous_driving_segmentation_model.pth')\n",
    "        print(f\"Saved best model at epoch {epoch+1}\")\n",
    "    \n",
    "    # Early stopping 체크\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "    \n",
    "    # 3 에폭마다 모델 저장\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.save(model.state_dict(), f'finetuned_autonomous_driving_segmentation_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved model at epoch {epoch+1}\")\n",
    "\n",
    "# 최종 모델 저장\n",
    "torch.save(model.state_dict(), 'final_finetuned_autonomous_driving_segmentation_model.pth')\n",
    "print(\"Fine-Tuning complete. Final model saved.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
