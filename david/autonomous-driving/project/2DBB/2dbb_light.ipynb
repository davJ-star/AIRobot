{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: conda: command not found\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: conda: command not found\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.2.78-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.0 (from ultralytics)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting matplotlib>=3.3.0 (from ultralytics)\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting pillow>=7.1.2 (from ultralytics)\n",
      "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyyaml>=5.3.1 (from ultralytics)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests>=2.23.0 (from ultralytics)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scipy>=1.4.1 (from ultralytics)\n",
      "  Downloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting tqdm>=4.64.0 (from ultralytics)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /home/mira/Desktop/KistAIRobot/others/lib/python3.11/site-packages (from ultralytics) (6.0.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Using cached ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Downloading fonttools-4.53.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mira/Desktop/KistAIRobot/others/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3.0->ultralytics)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mira/Desktop/KistAIRobot/others/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.4->ultralytics)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.23.0->ultralytics)\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting filelock (from torch>=1.8.0->ultralytics)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/Desktop/KistAIRobot/others/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Collecting sympy (from torch>=1.8.0->ultralytics)\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.8.0->ultralytics)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.8.0->ultralytics)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/mira/Desktop/KistAIRobot/others/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->ultralytics)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.8.0->ultralytics)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading ultralytics-8.2.78-py3-none-any.whl (869 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.0/869.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m797.2/797.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:03\u001b[0mm"
     ]
    }
   ],
   "source": [
    "# OpenCV\n",
    "%conda install -c conda-forge opencv\n",
    "\n",
    "# NumPy\n",
    "%conda install numpy\n",
    "\n",
    "# PyTorch (CUDA 버전에 따라 적절한 버전을 선택해야 합니다)\n",
    "%conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "# 또는 CUDA 지원을 추가하려면\n",
    "# conda install pytorch torchvision torchaudio cudatoolkit=11.8 -c pytorch\n",
    "\n",
    "# Ultralytics (YOLO)\n",
    "%pip install ultralytics\n",
    "\n",
    "# Albumentations\n",
    "%pip install albumentations\n",
    "\n",
    "# tqdm\n",
    "%conda install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data loading and preprocessing function\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels, data['image_name']\n",
    "\n",
    "# Data augmentation function\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# Convert to YOLO format\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# Prepare dataset function\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels, image_name = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # Original image and label processing\n",
    "            orig_yolo_labels = convert_to_yolo_format(boxes, labels, image.shape[:2])\n",
    "            orig_label_filename = f\"{json_file[:-5]}.txt\"\n",
    "            orig_label_path = os.path.join(labels_dir, orig_label_filename)\n",
    "            with open(orig_label_path, 'w') as f:\n",
    "                f.write('\\n'.join(orig_yolo_labels))\n",
    "            \n",
    "            # Augmented data\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            aug_yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # Store both original and augmented data\n",
    "            data.append((image_name, orig_label_path, aug_image, aug_yolo_labels))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Class mapping definition\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path, label_path, aug_image, aug_labels = item\n",
    "        \n",
    "        if aug_image is not None:  # Augmented data\n",
    "            image = cv2.cvtColor(aug_image, cv2.COLOR_BGR2RGB)\n",
    "            labels = aug_labels\n",
    "        else:  # Original image\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            with open(label_path, 'r') as f:\n",
    "                labels = f.read().splitlines()\n",
    "        \n",
    "        # Further preprocessing can be done here if needed\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "    \n",
    "    print(f\"Prepared {len(train_data)} training samples\")\n",
    "    print(f\"Prepared {len(val_data)} validation samples\")\n",
    "\n",
    "    # Create custom datasets\n",
    "    train_dataset = CustomDataset(train_data)\n",
    "    val_dataset = CustomDataset(val_data)\n",
    "    \n",
    "    # DataLoader for batching\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # YAML config file\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # Load YOLOv5 model\n",
    "    model = YOLO('yolov5n.pt')\n",
    "\n",
    "    # Model training\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=20,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        save_period=2,  # Save model every 2 epochs\n",
    "        weight_decay=0.005,  # L2 regularization\n",
    "        mosaic=0.5,  # Mosaic augmentation\n",
    "        mixup=0.2,  # Mixup augmentation\n",
    "        name='yolov5n_autonomous_driving',\n",
    "        early_stopping=True,  # Enable early stopping\n",
    "        patience=5,  # Patience for early stopping\n",
    "        save_best_only=True  # Save only the best model\n",
    "    )\n",
    "\n",
    "    # Model evaluation\n",
    "    results = model.val()\n",
    "\n",
    "    # Test data inference\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # Save test results\n",
    "    test_results.save('test_results')\n",
    "\n",
    "    # Export model (ONNX format)\n",
    "    model.export(format='onnx', simplify=True, opset=13, dynamic=True, half=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
