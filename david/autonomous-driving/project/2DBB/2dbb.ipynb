{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: torchaudio in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (2.4.0+cu118)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: numpy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.2.76-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (3.8.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (4.10.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (0.19.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (5.9.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.4->ultralytics)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.82)\n",
      "Requirement already satisfied: six>=1.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.2.76-py3-none-any.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, py-cpuinfo, tzdata, pandas, seaborn, ultralytics-thop, ultralytics\n",
      "Successfully installed pandas-2.2.2 py-cpuinfo-9.0.0 pytz-2024.1 seaborn-0.13.2 tzdata-2024.1 ultralytics-8.2.76 ultralytics-thop-2.0.0\n",
      "Requirement already satisfied: albumentations in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (1.4.10)\n",
      "Requirement already satisfied: numpy<2,>=1.24.4 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.14.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (4.12.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.5.1)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (2.8.2)\n",
      "Requirement already satisfied: albucore>=0.0.11 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (0.0.13)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albucore>=0.0.11->albumentations) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2024.7.24)\n",
      "Requirement already satisfied: packaging>=21 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install ultralytics\n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@749.230] global loadsave.cpp:241 findDecoder imread_('./training/images/S_DRG_230629_008_FC_049.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 메인 실행 코드\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training/labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training//output/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/labels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/images\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/output/val\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# 데이터 분할\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(json_dir, image_dir, output_dir)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     70\u001b[0m     json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(json_dir, json_file)\n\u001b[0;32m---> 71\u001b[0m     image, boxes, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# 데이터 증강\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     aug_image, aug_boxes, aug_labels \u001b[38;5;241m=\u001b[39m augment_data(image, boxes, labels)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(json_path, image_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 19\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m annotations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(json_dir, image_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    for json_file in os.listdir(json_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(json_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, image_dir)\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 이미지 저장\n",
    "            image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(output_dir, 'images', image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 레이블 저장\n",
    "            label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(output_dir, 'labels', label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('./training/labels', './training/images', './training//output/train')\n",
    "    val_data = prepare_dataset('./validation/labels', './validation/images', './validation/output/val')\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: ./training/images\n",
    "    val: ./validation/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('./test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('./save/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@6013.570] global loadsave.cpp:241 findDecoder imread_('training/images/S_DRG_230629_008_FC_049.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 메인 실행 코드\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# YAML 설정 파일 생성\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     69\u001b[0m     json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labels_dir, json_file)\n\u001b[0;32m---> 70\u001b[0m     image, boxes, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# 데이터 증강\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     aug_image, aug_boxes, aug_labels \u001b[38;5;241m=\u001b[39m augment_data(image, boxes, labels)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(json_path, image_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 19\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m annotations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 증강된 이미지 저장\n",
    "            aug_image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(images_dir, aug_image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 증강된 레이블 저장\n",
    "            aug_label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(labels_dir, aug_label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(aug_image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('test_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/mira/Desktop/KistAIRobot/david/autonomous-driving/project/2DBB\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_058.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_033.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_104.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_108.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_040.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_111.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_114.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_055.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_032.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_030.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_024.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_021.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_044.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_046.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_017.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_114.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FR_000.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_020.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_098.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_079.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_109.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_001.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_016.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_103.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_098.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_037.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_062.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_115.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_052.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_022.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_100.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_041.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_005.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_007.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_084.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_093.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_042.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_055.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_060.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_082.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_034.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_039.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_001.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_093.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_068.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_013.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_056.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_081.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_082.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_017.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_066.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_094.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_005.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_105.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_108.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_094.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_004.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_080.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_063.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_075.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_065.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_072.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_083.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_117.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_016.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_006.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_043.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_054.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_076.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_077.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_037.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_031.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_053.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_003.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_070.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_011.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_014.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_041.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_014.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_113.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_079.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_110.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_043.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_043.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_095.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_043.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_039.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_092.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_010.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_091.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_029.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_053.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_084.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_038.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_010.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_028.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_031.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_065.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_000.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_084.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_025.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_028.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FR_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_112.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_003.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_020.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_095.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_065.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_036.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_006.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_020.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_094.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_006.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_052.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_062.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_092.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_008.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_014.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_037.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_117.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_060.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_011.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_102.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_113.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_004.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_115.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_083.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_067.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_082.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_015.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_041.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_000.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_104.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_079.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_026.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_086.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_055.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_042.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_021.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_053.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_113.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_041.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_030.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_044.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_042.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_011.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_038.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_004.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_057.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_112.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_078.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_025.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_100.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_071.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_040.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_078.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_059.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_071.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_086.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_116.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_033.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_101.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_067.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_010.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_115.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FR_010.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_006.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_073.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_036.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_083.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_066.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_086.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_031.jpg\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent working directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m val_data \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# 데이터 준비 결과 출력\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 증강된 레이블 저장\u001b[39;00m\n\u001b[1;32m     98\u001b[0m aug_label_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maug_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_label_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43myolo_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(aug_image_filename)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    \n",
    "    # 파일 존재 여부 확인\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # 이미지 로드 실패 확인\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            if image is None:\n",
    "                continue  # 이미지 로드에 실패한 경우 다음 파일로 넘어갑니다.\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 증강된 이미지 저장\n",
    "            aug_image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(images_dir, aug_image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 증강된 레이블 저장\n",
    "            aug_label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(labels_dir, aug_label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(aug_image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 현재 작업 디렉토리 출력\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    \n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "    \n",
    "    # 데이터 준비 결과 출력\n",
    "    print(f\"Prepared {len(train_data)} training samples\")\n",
    "    print(f\"Prepared {len(val_data)} validation samples\")\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('test_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolov5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myolov5\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 사용자 정의 데이터셋 클래스\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAutonomousDrivingDataset\u001b[39;00m(Dataset):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov5'"
     ]
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import yolov5\n",
    "\n",
    "# 사용자 정의 데이터셋 클래스\n",
    "class AutonomousDrivingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.label_dir = os.path.join(root_dir, 'labels')\n",
    "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.json'))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_data = json.load(f)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in label_data['Annotation']:\n",
    "            x_min, y_min, x_max, y_max = annotation['data']\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(self.class_to_index(annotation['class_name']))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, bboxes=boxes, class_labels=labels)\n",
    "            image = augmented['image']\n",
    "            target['boxes'] = torch.tensor(augmented['bboxes'])\n",
    "            target['labels'] = torch.tensor(augmented['class_labels'])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    @staticmethod\n",
    "    def class_to_index(class_name):\n",
    "        classes = ['car', 'bus', 'truck', 'special vehicle', 'motorcycle', 'bicycle', \n",
    "                   'personal mobility', 'person', 'Traffic_light', 'Traffic_sign']\n",
    "        return classes.index(class_name)\n",
    "\n",
    "# 데이터 증강 및 전처리\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=640, width=640),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.MotionBlur(p=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = AutonomousDrivingDataset('2DBB/training', transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# YOLOv5 모델 로드 및 설정\n",
    "model = yolov5.load('yolov5s.pt')\n",
    "model.train()\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "hyp = {\n",
    "    'lr0': 0.01,\n",
    "    'lrf': 0.1,\n",
    "    'momentum': 0.937,\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3.0,\n",
    "    'warmup_momentum': 0.8,\n",
    "    'warmup_bias_lr': 0.1,\n",
    "    'box': 0.05,\n",
    "    'cls': 0.5,\n",
    "    'cls_pw': 1.0,\n",
    "    'obj': 1.0,\n",
    "    'obj_pw': 1.0,\n",
    "    'iou_t': 0.20,\n",
    "    'anchor_t': 4.0,\n",
    "    'fl_gamma': 0.0,\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.7,\n",
    "    'hsv_v': 0.4,\n",
    "    'degrees': 0.0,\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'shear': 0.0,\n",
    "    'perspective': 0.0,\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.5,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.0\n",
    "}\n",
    "\n",
    "# 모델 학습\n",
    "results = model.train(data='data.yaml', epochs=100, imgsz=640, batch_size=16, hyp=hyp)\n",
    "\n",
    "# 모델 저장\n",
    "model.save('best_model.pt')\n",
    "\n",
    "# 검증 데이터셋에 대한 성능 평가\n",
    "val_dataset = AutonomousDrivingDataset('2DBB/validation', transform=train_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "results = model.val(data='data.yaml', dataloader=val_loader)\n",
    "print(results)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/mira/Desktop/KistAIRobot/david/autonomous-driving/project/2DBB\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_058.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_033.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_104.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_108.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_040.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_111.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_114.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_055.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels, data['image_name']\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels, image_name = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # 원본 이미지 및 레이블 처리\n",
    "            orig_yolo_labels = convert_to_yolo_format(boxes, labels, image.shape[:2])\n",
    "            orig_label_filename = f\"{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(labels_dir, orig_label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(orig_yolo_labels))\n",
    "            \n",
    "            data.append(os.path.join(images_dir, image_name))\n",
    "            \n",
    "            # 데이터 증강 및 처리 (저장하지 않고 메모리에서 처리)\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            aug_yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 증강된 데이터를 메모리에 저장\n",
    "            data.append((aug_image, aug_yolo_labels))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 커스텀 데이터셋 클래스\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        if isinstance(item, tuple):  # 증강된 데이터\n",
    "            image, labels = item\n",
    "        else:  # 원본 이미지 경로\n",
    "            image = cv2.imread(item)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            label_path = item.replace('images', 'labels').replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "            with open(label_path, 'r') as f:\n",
    "                labels = f.read().splitlines()\n",
    "        \n",
    "        # 이미지와 레이블을 YOLO가 기대하는 형식으로 변환\n",
    "        # (이 부분은 YOLO 모델의 요구사항에 따라 조정해야 할 수 있습니다)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    \n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "    \n",
    "    print(f\"Prepared {len(train_data)} training samples\")\n",
    "    print(f\"Prepared {len(val_data)} validation samples\")\n",
    "\n",
    "    # 커스텀 데이터셋 생성\n",
    "    train_dataset = CustomDataset(train_data)\n",
    "    val_dataset = CustomDataset(val_data)\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 경량화된 모델 로드 (YOLOv5n)\n",
    "    model = YOLO('yolov5n.pt')\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=20,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        save_period=2,  # 2 에폭마다 모델 저장\n",
    "        weight_decay=0.0005,  # L2 정규화\n",
    "        mosaic=0.5,  # 모자이크 증강\n",
    "        mixup=0.2,  # Mixup 증강\n",
    "        name='yolov5n_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('test_results')\n",
    "\n",
    "    # 모델 내보내기 (ONNX 포맷으로 경량화 및 양자화)\n",
    "    model.export(format='onnx', simplify=True, opset=13, dynamic=True, half=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kistAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
