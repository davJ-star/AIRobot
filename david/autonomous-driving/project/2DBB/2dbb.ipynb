{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: torchaudio in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (2.4.0+cu118)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: numpy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.2.76-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (3.8.4)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (4.10.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (0.19.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from ultralytics) (5.9.0)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pandas>=1.1.4 (from ultralytics)\n",
      "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.1.4->ultralytics)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.4->ultralytics)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: filelock in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (69.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.82)\n",
      "Requirement already satisfied: six>=1.5 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.2.76-py3-none-any.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, py-cpuinfo, tzdata, pandas, seaborn, ultralytics-thop, ultralytics\n",
      "Successfully installed pandas-2.2.2 py-cpuinfo-9.0.0 pytz-2024.1 seaborn-0.13.2 tzdata-2024.1 ultralytics-8.2.76 ultralytics-thop-2.0.0\n",
      "Requirement already satisfied: albumentations in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (1.4.10)\n",
      "Requirement already satisfied: numpy<2,>=1.24.4 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.14.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (4.12.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (1.5.1)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (2.8.2)\n",
      "Requirement already satisfied: albucore>=0.0.11 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (0.0.13)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from albucore>=0.0.11->albumentations) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (2024.7.24)\n",
      "Requirement already satisfied: packaging>=21 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mira/anaconda3/envs/kistAI/lib/python3.12/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install ultralytics\n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@749.230] global loadsave.cpp:241 findDecoder imread_('./training/images/S_DRG_230629_008_FC_049.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 메인 실행 코드\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training/labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./training//output/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/labels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/images\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./validation/output/val\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# 데이터 분할\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 71\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(json_dir, image_dir, output_dir)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     70\u001b[0m     json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(json_dir, json_file)\n\u001b[0;32m---> 71\u001b[0m     image, boxes, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# 데이터 증강\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     aug_image, aug_boxes, aug_labels \u001b[38;5;241m=\u001b[39m augment_data(image, boxes, labels)\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(json_path, image_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 19\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m annotations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(json_dir, image_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "\n",
    "    data = []\n",
    "    for json_file in os.listdir(json_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(json_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, image_dir)\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 이미지 저장\n",
    "            image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(output_dir, 'images', image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 레이블 저장\n",
    "            label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(output_dir, 'labels', label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('./training/labels', './training/images', './training//output/train')\n",
    "    val_data = prepare_dataset('./validation/labels', './validation/images', './validation/output/val')\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: ./training/images\n",
    "    val: ./validation/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('./test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('./save/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@6013.570] global loadsave.cpp:241 findDecoder imread_('training/images/S_DRG_230629_008_FC_049.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 메인 실행 코드\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     val_data \u001b[38;5;241m=\u001b[39m prepare_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# YAML 설정 파일 생성\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     69\u001b[0m     json_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labels_dir, json_file)\n\u001b[0;32m---> 70\u001b[0m     image, boxes, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# 데이터 증강\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     aug_image, aug_boxes, aug_labels \u001b[38;5;241m=\u001b[39m augment_data(image, boxes, labels)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(json_path, image_dir)\u001b[0m\n\u001b[1;32m     17\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 19\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m annotations \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 증강된 이미지 저장\n",
    "            aug_image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(images_dir, aug_image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 증강된 레이블 저장\n",
    "            aug_label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(labels_dir, aug_label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(aug_image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('test_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/mira/Desktop/KistAIRobot/david/autonomous-driving/project/2DBB\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_058.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_033.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_104.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_108.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_040.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_111.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_114.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_055.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_032.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_030.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_024.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_021.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_044.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_046.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_017.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_114.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FR_000.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_020.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_098.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_079.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_109.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_001.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_016.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_103.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_098.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_037.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_035.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_062.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_115.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_052.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_022.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_100.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_041.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_005.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_007.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_084.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_107.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_093.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_042.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_055.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_060.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_082.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_034.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_039.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_001.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_093.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_068.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_013.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_050.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_099.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_056.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_081.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_064.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_082.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_017.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_066.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_094.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_005.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_105.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_108.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_094.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_004.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_012.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_080.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_063.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_075.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RR_065.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_072.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_083.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FR_117.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_049.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_016.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_006.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FL_085.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_106.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_043.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_054.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_076.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_077.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_037.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FL_031.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_053.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_RL_003.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_018.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_070.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RR_069.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_RL_011.jpg\n",
      "Image file not found: training/images/S_DRG_230629_009_FC_088.jpg\n",
      "Image file not found: training/images/S_DRG_230629_008_FC_014.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations import (\n",
    "    Compose, RandomBrightnessContrast, HueSaturationValue, GaussNoise,\n",
    "    HorizontalFlip, RandomCrop, Rotate, ShiftScaleRotate\n",
    ")\n",
    "\n",
    "# 데이터 로딩 및 전처리 함수\n",
    "def load_and_preprocess_data(json_path, image_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    image_path = os.path.join(image_dir, data['image_name'])\n",
    "    \n",
    "    # 파일 존재 여부 확인\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # 이미지 로드 실패 확인\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    annotations = data['Annotation']\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for ann in annotations:\n",
    "        boxes.append(ann['data'])\n",
    "        labels.append(ann['class_name'])\n",
    "    \n",
    "    return image, np.array(boxes), labels\n",
    "\n",
    "# 박스 정규화 함수\n",
    "def normalize_boxes(boxes, image_size):\n",
    "    return boxes / np.array([image_size[1], image_size[0], image_size[1], image_size[0]])\n",
    "\n",
    "# 데이터 증강 함수\n",
    "def augment_data(image, boxes, labels):\n",
    "    augmentations = Compose([\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        HueSaturationValue(p=0.5),\n",
    "        GaussNoise(p=0.3),\n",
    "        HorizontalFlip(p=0.5),\n",
    "        RandomCrop(height=image.shape[0], width=image.shape[1], p=0.3),\n",
    "        Rotate(limit=10, p=0.3),\n",
    "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    \n",
    "    augmented = augmentations(image=image, bboxes=boxes, labels=labels)\n",
    "    return augmented['image'], np.array(augmented['bboxes']), augmented['labels']\n",
    "\n",
    "# YOLO 형식으로 레이블 변환\n",
    "def convert_to_yolo_format(boxes, labels, image_size):\n",
    "    yolo_labels = []\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x_center = (box[0] + box[2]) / 2 / image_size[1]\n",
    "        y_center = (box[1] + box[3]) / 2 / image_size[0]\n",
    "        width = (box[2] - box[0]) / image_size[1]\n",
    "        height = (box[3] - box[1]) / image_size[0]\n",
    "        class_id = class_mapping[label]\n",
    "        yolo_labels.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "    return yolo_labels\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(data_dir):\n",
    "    images_dir = os.path.join(data_dir, 'images')\n",
    "    labels_dir = os.path.join(data_dir, 'labels')\n",
    "    \n",
    "    data = []\n",
    "    for json_file in os.listdir(labels_dir):\n",
    "        if json_file.endswith('.json'):\n",
    "            json_path = os.path.join(labels_dir, json_file)\n",
    "            image, boxes, labels = load_and_preprocess_data(json_path, images_dir)\n",
    "            \n",
    "            if image is None:\n",
    "                continue  # 이미지 로드에 실패한 경우 다음 파일로 넘어갑니다.\n",
    "            \n",
    "            # 데이터 증강\n",
    "            aug_image, aug_boxes, aug_labels = augment_data(image, boxes, labels)\n",
    "            \n",
    "            # YOLO 형식으로 변환\n",
    "            yolo_labels = convert_to_yolo_format(aug_boxes, aug_labels, aug_image.shape[:2])\n",
    "            \n",
    "            # 증강된 이미지 저장\n",
    "            aug_image_filename = f\"aug_{json_file[:-5]}.jpg\"\n",
    "            cv2.imwrite(os.path.join(images_dir, aug_image_filename), cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # 증강된 레이블 저장\n",
    "            aug_label_filename = f\"aug_{json_file[:-5]}.txt\"\n",
    "            with open(os.path.join(labels_dir, aug_label_filename), 'w') as f:\n",
    "                f.write('\\n'.join(yolo_labels))\n",
    "            \n",
    "            data.append(aug_image_filename)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 클래스 매핑 정의\n",
    "class_mapping = {\n",
    "    'car': 0, 'bus': 1, 'truck': 2, 'special vehicle': 3,\n",
    "    'motorcycle': 4, 'bicycle': 5, 'personal mobility': 6,\n",
    "    'person': 7, 'Traffic_light': 8, 'Traffic_sign': 9\n",
    "}\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 현재 작업 디렉토리 출력\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    \n",
    "    # 데이터 준비\n",
    "    train_data = prepare_dataset('training')\n",
    "    val_data = prepare_dataset('validation')\n",
    "    \n",
    "    # 데이터 준비 결과 출력\n",
    "    print(f\"Prepared {len(train_data)} training samples\")\n",
    "    print(f\"Prepared {len(val_data)} validation samples\")\n",
    "\n",
    "    # YAML 설정 파일 생성\n",
    "    yaml_content = f\"\"\"\n",
    "    train: training/images\n",
    "    val: validation/images\n",
    "    test: test/images\n",
    "\n",
    "    nc: {len(class_mapping)}\n",
    "    names: {list(class_mapping.keys())}\n",
    "    \"\"\"\n",
    "\n",
    "    with open('dataset.yaml', 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "    # 모델 로드 및 학습\n",
    "    model = YOLO('yolov5s.pt')  # 사전 학습된 YOLOv5s 모델 로드\n",
    "\n",
    "    # 모델 학습\n",
    "    results = model.train(\n",
    "        data='dataset.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='yolov5_autonomous_driving'\n",
    "    )\n",
    "\n",
    "    # 모델 평가\n",
    "    results = model.val()\n",
    "\n",
    "    # 테스트 데이터에 대한 예측\n",
    "    test_results = model('test/images')\n",
    "\n",
    "    # 결과 저장\n",
    "    test_results.save('test_results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kistAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
